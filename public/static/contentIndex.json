{"1-Devops/Docker-Mastery/0-course-overview":{"slug":"1-Devops/Docker-Mastery/0-course-overview","filePath":"1-Devops/Docker-Mastery/0-course-overview.md","title":"0. Course Overview and Index","links":["1-Devops/Docker-Mastery/1-docker-images","1-Devops/Docker-Mastery/2-docker-containers","1-Devops/Docker-Mastery/3-docker-networking","1-Devops/Docker-Mastery/4-docker-volumes","1-Devops/Docker-Mastery/5-dockerfile-mastery","1-Devops/Docker-Mastery/6-multi-stage-docker-builds","1-Devops/Docker-Mastery/7-docker-security-essentials","1-Devops/Docker-Mastery/8-docker-registries-and-tagging","1-Devops/Docker-Mastery/9-docker-compose","1-Devops/Docker-Mastery/10-debugging-docker","1-Devops/Docker-Mastery/11-from-docker-to-orchestrators","1-Devops/Docker-Mastery/","1-Devops/","/"],"tags":[],"content":"This course builds a practical mental model of Docker from first principles\nand then connects it to real-world DevOps workflows.\nEach section below links to a focused module that you can study and practice\nindependently.\n\n1. Docker Images â€“ How the Lego Bricks of Containers Really Work\nğŸ‘‰ Go to: Docker Images\nYou will learn:\n\n\nImage vs container vs registry\n\nImage as an immutable blueprint\nContainer as a running process plus writable layer\nRegistry as a Git-like store for built images\n\n\n\nLayered filesystems and why each instruction matters\n\nHow every Dockerfile instruction creates a new layer\nWhy RUN apt-get update &amp;&amp; apt-get install ... vs multiple RUNs changes cache behavior\nRefactoring a bad Dockerfile into a cache-friendly one\n\n\n\nBuild, tag, and push workflow\n\nMental model of docker build: context â†’ Dockerfile â†’ layers\nSemantic tagging strategy (1.0.0, 1.0.0-prod, latest)\nEnd-to-end flow from local build to private registry\n\n\n\nInspecting and understanding images\n\nUsing docker inspect and docker history\nDetecting bloated layers\nComparing openjdk, temurin, and distroless\n\n\n\nCleaning up images\n\nDangling images and safe pruning\nWhen and how to use docker image prune\n\n\n\nPractical best practices\n\nSmall base images\nDeterministic Dockerfiles\nNo secrets in images\n\n\n\n\n2. Containers â€“ Processes, Not Tiny VMs\nğŸ‘‰ Go to: Docker Containers\nYou will learn:\n\nContainer = process + isolation\nContainer lifecycle (create â†’ start â†’ stop â†’ remove)\nReal-world docker run usage\nLogs, exec, inspect, and stats\nCPU and memory limits\nPractical rules of thumb\n\n\n3. Docker Networking â€“ Making Containers Talk Like Grown-Ups\nğŸ‘‰ Go to: Docker Networking\nYou will learn:\n\nNetwork namespaces and bridges\nBuilt-in Docker network drivers\nUser-defined bridges and DNS\nPort publishing vs internal ports\nDebugging connectivity issues\nMapping Docker networking to Kubernetes\n\n\n4. Volumes â€“ Keeping Data Alive When Containers Die\nğŸ‘‰ Go to: Docker Volumes\nYou will learn:\n\nEphemeral container filesystems\nNamed volumes vs bind mounts vs tmpfs\nPersistent databases in Docker\nBind mounts for development\nVolume inspection and backups\nProduction storage rules\n\n\n5. Dockerfile Instructions â€“ Writing Dockerfiles Like an Engineer\nğŸ‘‰ Go to: Dockerfile Mastery\nYou will learn:\n\nDockerfile as a deterministic build recipe\nCore instructions and intent\nCOPY vs ADD\nENV vs ARG\nCMD vs ENTRYPOINT\nSecurity-aware Dockerfiles\nRefactoring bad Dockerfiles\n\n\n6. Multi-Stage Builds â€“ Shrinking Images and Attack Surface\nğŸ‘‰ Go to: Multi-Stage Docker Builds\nYou will learn:\n\nWhy single-stage images are dangerous\nBuild vs runtime separation\nNode, Java, and Go examples\nCache optimization\nSecurity benefits\nMigrating legacy Dockerfiles\n\n\n7. Docker Security â€“ Baseline Guardrails for Devs and DevOps\nğŸ‘‰ Go to: Docker Security Essentials\nYou will learn:\n\nContainer threat model\nNon-root users\nCapabilities and seccomp\nImage hygiene\nRuntime hardening flags\nSecrets handling\n\n\n8. Registries and Tagging â€“ Versioning Containers Like Real Software\nğŸ‘‰ Go to: Docker Registries &amp; Tagging\nYou will learn:\n\nRegistries as artifact repositories\nTagging strategies that scale\nCI/CD image workflows\nImage promotion across environments\nAuthentication and access control\n\n\n9. Docker Compose â€“ Local Microservices Without Losing Your Mind\nğŸ‘‰ Go to: Docker Compose\nYou will learn:\n\nWhy Compose exists\nServices, networks, and volumes\nMulti-service local stacks\nDeveloper workflows\nBest practices\nBridge to Kubernetes\n\n\n10. Troubleshooting and Debugging â€“ A Systematic Playbook\nğŸ‘‰ Go to: Debugging Docker\nYou will learn:\n\nDebugging mindset\nContainers that wonâ€™t start\nApps not reachable\nInter-container connectivity issues\nPerformance and OOM debugging\nBuilding a reusable debug SOP\n\n\n11. Orchestration Hooks â€“ From Docker to Kubernetes\nğŸ‘‰ Go to: From Docker to Orchestrators\nYou will learn:\n\nWhy Docker alone isnâ€™t enough\nDocker â†’ Kubernetes mental model mapping\nSwarm overview\nImage and rollout considerations in k8s\nMigration path from Compose to Kubernetes\n\n\nâ† Back to Docker Mastery\nâ† Back to DevOps Track\nâ† Back to Courses Home"},"1-Devops/Docker-Mastery/1-docker-images":{"slug":"1-Devops/Docker-Mastery/1-docker-images","filePath":"1-Devops/Docker-Mastery/1-docker-images.md","title":"1. Docker Images","links":[],"tags":[],"content":"If containers are lightweight processes, images are the immutable blueprints\nthat define what those processes look like at runtime.\nThis article walks from mental models to real Dockerfile behavior.\n1. Mental Model: Image vs Container vs Registry\nBefore touching commands, fix this mental picture in your head:\n\nImage\n\nA readâ€‘only, versioned filesystem template plus metadata.\nThink: â€œfrozen snapshot of a root filesystem + configâ€.\n\n\nContainer\n\nA running (or stopped) Linux process that uses an image as its root filesystem, plus a small writable layer on top.\nThink: â€œimage + runtime stateâ€.\n\n\nRegistry\n\nA remote storage for images, similar to a Git server for code.\nDocker Hub, ECR, GCR, GitHub Container Registry, etc.\n\n\n\nWorkflow in one sentence:\nYou build an image locally â†’ tag it â†’ push it to a registry â†’ pull and run it on other machines (or clusters).\n\n2. Image Internals: Layers and the Build Graph\nDocker images are not single files; theyâ€™re stacks of layers, usually implemented via a union filesystem. Each layer:\n\nRepresents a change to the filesystem (add files, remove files, modify files).\nIs identified by a content hash.\nIs immutable once created.\nCan be shared between images to save space and speed up pulls.\n\nWhen you write a Dockerfile:\n\nFROM eclipse-temurin:21-jre-alpine\n\nWORKDIR /app\nCOPY app.jar /app/app.jar\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n\n\nConceptually, Docker does something like this:\n\nStart from the base image eclipse-temurin:21-jre-alpine (many layers already).\nAdd a layer that creates/sets WORKDIR /app.\nAdd a layer that copies app.jar.\nAdd metadata for the CMD.\n\nEach instruction produces a new layer if it changes the filesystem. That layering is exactly what powers Dockerâ€™s build cache and efficient distribution.\n\n3. Build Cache: Why Instruction Order Matters\nThe Docker build cache works from top to bottom of your Dockerfile:\n\nFor each instruction, Docker checks: â€œHave I seen this same instruction with the same inputs before?â€\nIf yes, it can reuse the previously built layer instead of rebuilding it.\nIf no (e.g., different files, different command), it must rebuild from that point downward.\n\nThat means:\n\nIf you put COPY . . near the top, any change in your source tree invalidates cache for all later steps, including heavy dependency installs.\nIf you separate dependency steps from source code, you can avoid reâ€‘downloading dependencies on every small code change.\n\nExample: Java Maven app.\nSuboptimal Dockerfile:\nFROM eclipse-temurin:21-jdk-alpine\n\nWORKDIR /app\nCOPY . .\nRUN mvn -q -B package -DskipTests\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;target/app.jar&quot;]\n\n\nEvery time you change any file in the repo, COPY . . changes â†’ cache invalidation from there down â†’ Maven redownloads stuff and rebuilds.\nBetter, cacheâ€‘friendly multiâ€‘stage Dockerfile:\nFROM eclipse-temurin:21-jdk-alpine AS build\nWORKDIR /app\n\n# 1. Copy only dependency descriptors and warm cache\nCOPY pom.xml .\nRUN mvn -q -B dependency:go-offline\n\n# 2. Copy source code and build\nCOPY src ./src\nRUN mvn -q -B package -DskipTests\n\n# 3. Runtime image\nFROM eclipse-temurin:21-jre-alpine\nWORKDIR /app\nCOPY --from=build /app/target/app.jar app.jar\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n\n\nNow:\n\nChanging code in src does not invalidate the dependency cache step.\nOnly the final build step reâ€‘runs, making rebuilds much faster.\n\nThis â€œdependency layer before source layerâ€ pattern is universal: Node, Python, Go, Java, etc.\n\n4. Core Commands: List, Build, Tag, Push, Inspect\n4.1 Listing Images\nSee whatâ€™s on your machine:\ndocker images\n# or\ndocker image ls\n\n\nYouâ€™ll see columns like:\n\nREPOSITORY (myorg/payment-service)\nTAG (1.0.0, latest)\nIMAGE ID\nCREATED\nSIZE\n\nGood hygiene: periodically scan this list and prune unused images.\n\n4.2 Building Images\nBasic build:\ndocker build -t myorg/payment-service:1.0.0 .\n\nKey points:\n\n. is the build context: everything under this directory is sent to the Docker daemon.\nAvoid sending huge directories (node_modules, target, .git) unless needed â†’ use .dockerignore.\n\nDisable cache when you really want fresh layers:\ndocker build --no-cache -t myorg/payment-service:1.0.0 .\n\nYouâ€™ll rarely want --no-cache in normal dev; itâ€™s mainly for debugging or when the cache gets confusing.\n\n4.3 Tagging Images\nTags are just labels pointing to a specific image ID.\nCommon patterns:\n# Changing tag locally\ndocker tag myorg/payment-service:1.0.0 myorg/payment-service:latest\ndocker tag myorg/payment-service:1.0.0 myorg/payment-service:1.0.0-prod\n\n\nMentally treat tags like Git branches pointing to commits:\n\nlatest is not special; itâ€™s just a tag.\nYou decide what latest means (usually â€œmost stable releaseâ€ or â€œmost recent buildâ€).\n\n\n4.4 Pushing &amp; Pulling (Registries)\nOnce tagged correctly, push to a registry:\n# Login once (if required)\ndocker login my-registry.example.com\n\n# Tag for registry namespace\ndocker tag myorg/payment-service:1.0.0 \\\n  my-registry.example.com/myorg/payment-service:1.0.0\n\n# Push\ndocker push my-registry.example.com/myorg/payment-service:1.0.0\n\n\nOn another machine (or your CI/CD):\ndocker pull my-registry.example.com/myorg/payment-service:1.0.0\ndocker run -d -p 8080:8080 my-registry.example.com/myorg/payment-service:1.0.0\n\n\nYouâ€™ve now decoupled build (anywhere) from run (anywhere else).\n\n4.5 Inspecting Images and Their Layers\nTo see the full metadata:\ndocker inspect myorg/payment-service:1.0.0\n\nUseful sections:\n\nConfig.Env â†’ default environment variables baked into the image.\nConfig.Cmd &amp; Config.Entrypoint â†’ what runs by default.\nRootFS.Layers â†’ the list of layer digests.\n\nTo see Dockerfile history:\ndocker history myorg/payment-service:1.0.0\n\nThis shows:\n\nEach layerâ€™s size.\nThe instruction that produced it (when available).\nWhich layers are huge and could be optimized.\n\nYou can often spot mistakes like:\n\nGiant RUN layer that includes package caches.\nAccidentally copying the entire repo (COPY . .) when only a few directories are needed.\n\n\n5. Image Size: Why It Matters and How to Shrink It\nBig images hurt you in several ways:\n\nSlower pushes and pulls (more network usage).\nSlower deployments in Kubernetes clusters.\nMore disk usage on every node.\nLarger attack surface: more packages â†’ more CVEs.\n\n5.1 Choose the Right Base Image\nFor example:\n\nubuntu or debian â†’ full distribution, useful for tooling but heavy.\nalpine â†’ very small, muslâ€‘based, good for many apps but not all (e.g., some JVM or glibcâ€‘dependent tools need tweaks).\nLanguageâ€‘specific slim variants (python:3.13-slim, openjdk:21-jre-slim).\n\nIf your app is a Spring Boot service:\n\nA typical progression: openjdk:21-jre â†’ eclipse-temurin:21-jre-alpine â†’ maybe even distroless Java images later.\n\n5.2 Clean Up After Package Installs\nIn single RUN instructions, chain commands so you can remove caches in the same layer:\nRUN apk add --no-cache curl\n\n\nFor aptâ€‘based images:\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends curl &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\n\nIf you donâ€™t clean up, the package index stays in the layer and bloats your image.\n5.3 Multi-Stage Builds to Strip Tools\nAs shown earlier, multiâ€‘stage builds keep compilers and build tools in a separate stage, and copy only artifacts to the final runtime image.\nFor example, Node:\nFROM node:22-alpine AS build\nWORKDIR /app\nCOPY package*.json ./\nRUN npm ci\nCOPY . .\nRUN npm run build\n\nFROM nginx:1.25-alpine\nCOPY --from=build /app/dist /usr/share/nginx/html\nEXPOSE 80\nCMD [&quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot;]\n\n\nHere:\n\nNo Node, npm, or dev dependencies remain in the final image.\nFinal image is just Nginx + static files.\n\n\n6. Tagging Strategy: Knowing Whatâ€™s in Prod\nBad pattern: everyone just uses :latest everywhere, and no one knows what commit is actually running in production.\nBetter approach:\n\nAlways tag images with:\n\nSemantic version: 1.0.0.\nBuild identifier: 1.0.0-20260130.1.\nGit SHA: app:git-abc1234 (even if only for internal use).\n\n\n\nExample flow for a CI build:\n\n\nBuild the image from commit abc1234.\n\n\nTag:\n\nmyorg/payment-service:1.0.0\nmyorg/payment-service:1.0.0-abc1234\nmyorg/payment-service:git-abc1234\n\n\n\nPush all tags.\n\n\nDeploy a specific tag (1.0.0-abc1234) in staging.\n\n\nPromote the same tag to production (retag or reuse directly) instead of rebuilding.\n\n\nThis makes it easy to answer â€œwhat exact code is running in prod?â€ and to roll back by deploying a previously known tag.\n\n7. Cleanup and Disk Management\nOver time, your Docker host accumulates:\n\nOld images.\nDangling images (no tags pointing to them).\nBuild cache for images you donâ€™t use anymore.\n\nCommands youâ€™ll use to stay sane:\n# Remove unused images (no containers use them)\ndocker image prune\n\n# More aggressive: remove all images not referenced by any container\ndocker image prune -a\n\n# Remove unused containers, networks, images (and optionally volumes if you add flags)\ndocker system prune\ndocker system prune -a\n\n\nUse aggressive flags (-a) with care, especially on shared or production systems.\nIn a dev environment, a periodic docker system prune -a is fine as long as you know youâ€™ll be reâ€‘pulling images.\n\n8. Security Basics for Images\nEven at the image level, you can make security better or worse.\nKey principles:\n\nMinimal base: fewer packages, fewer vulnerabilities.\nNo secrets baked into images:\n\nNever COPY .env or embed passwords as ENV variables in the Dockerfile.\n\n\nNonâ€‘root where possible:\n\nCreate a dedicated user and USER switch to it in the Dockerfile.\n\n\n\nExample:\nFROM eclipse-temurin:21-jre-alpine\n\n# Create user and group\nRUN addgroup -S app &amp;&amp; adduser -S app -G app\n\nWORKDIR /app\nCOPY app.jar /app/app.jar\n\nUSER app\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n\n\nThis wonâ€™t make you bulletproof, but itâ€™s a baseline: an exploit in your app has fewer permissions inside the container.\n\n9. Putting It Together: A Typical Spring Boot Image\nHereâ€™s a complete example that combines most of the ideas above.\n# Build stage\nFROM eclipse-temurin:21-jdk-alpine AS build\nWORKDIR /app\n\n# Dependencies\nCOPY pom.xml .\nRUN mvn -q -B dependency:go-offline\n\n# Source and build\nCOPY src ./src\nRUN mvn -q -B package -DskipTests\n\n# Runtime stage\nFROM eclipse-temurin:21-jre-alpine\n\n# Create non-root user\nRUN addgroup -S app &amp;&amp; adduser -S app -G app\n\nWORKDIR /app\nCOPY --from=build /app/target/app.jar app.jar\n\nUSER app\nEXPOSE 8080\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n\n\nThis gives you:\n\nCacheâ€‘friendly builds.\nSmaller runtime image (no Maven/JDK).\nNonâ€‘root user.\nExplicit port.\n\nFrom there:\ndocker build -t myorg/orders-api:1.0.0 .\ndocker tag myorg/orders-api:1.0.0 myorg/orders-api:latest\ndocker run -d --name orders-api -p 8080:8080 myorg/orders-api:1.0.0\n\n\nYou now have a wellâ€‘structured image lifecycle, instead of random copyâ€‘paste Dockerfiles.\n"},"1-Devops/Docker-Mastery/10-debugging-docker":{"slug":"1-Devops/Docker-Mastery/10-debugging-docker","filePath":"1-Devops/Docker-Mastery/10-debugging-docker.md","title":"10. Debugging Docker","links":[],"tags":[],"content":"When something breaks in Docker, resist the urge to â€œtry random commands until it works.â€ Containers are just Linux processes with extra plumbing, so you debug them the same way: check if they exist, if they run, what they log, what they listen on, and what they can reach.\nThis playbook gives you aÂ sequenceÂ to follow so you donâ€™t miss obvious issues.\n\n1. Mindset: treat containers like Linux processes\nBefore touching Dockerâ€‘specific tricks, anchor this:\n\nA container is aÂ processÂ (or a few) on the host, with:\n\nIts own filesystem view (from the image).\nIts own network namespace (its own IP/ports).\nResource limits via cgroups.\n\n\n\nSo, for any problem, ask in order:\n\nDoes the containerÂ exist? (docker ps -a)\nIs itÂ running? (docker ps)\nWhat is itsÂ status and exit code? (docker inspect)\nWhat do theÂ logsÂ say? (docker logs)\nHow areÂ CPU, memory, portsÂ behaving? (docker stats,Â ss/netstat,Â topÂ inside).\n\nDonâ€™t jump straight to exotic tools. 80% of issues fall into:\n\nWrong command / missing binary.\nPermissions.\nWrong ports.\nNetwork miswiring.\nResource exhaustion.\n\n\n2. Container wonâ€™t start\nSymptom: you runÂ docker run ..., it exits immediately, orÂ docker compose upÂ shows services flapping.\n2.1 Check status and exit code\nList recently exited container:\nbash\ndocker ps -a --latest\nInspect its state:\nbash\ndocker inspect &lt;container&gt; --format &#039;{{.State.Status}} {{.State.ExitCode}} {{.State.OOMKilled}} {{.State.Error}}&#039;\n\nStatusÂ â€“ e.g.,Â exited.\nExitCodeÂ â€“Â 0Â means â€œclean exit,â€ nonâ€‘zero indicates error.\nOOMKilledÂ â€“Â trueÂ if the kernel killed it for memory.\n\n2.2 Look at logs\ndocker logs &lt;container&gt;\n# Or follow if it keeps flapping:\ndocker logs -f &lt;container&gt;\n\nCommon issues visible here:\n\nCommand not found:\n\nError like:Â executable file not found in $PATH.\nYour CMD/ENTRYPOINT points to a script/binary that doesnâ€™t exist or isnâ€™t executable.\n\n\nPermission denied:\n\nRunning a script without exec permission.\nWriting to a path the user canâ€™t access.\n\n\nMissing environment/config:\n\nApplication fails early because required env vars or config files arenâ€™t set.\n\n\n\n2.3 Reproduce interactively\nIf logs are unclear, start a container with an interactive shell using the same image:\ndocker run --rm -it &lt;image&gt; sh\n# or bash if available\n\nFrom there, manually run the command your Dockerfile uses as ENTRYPOINT/CMD:\njava -jar app.jar\n# or\n./start.sh\n\nYouâ€™ll see errors in real time and can inspect the filesystem to understand whatâ€™s missing.\n\n3. Container runs, but app is unreachable\nSymptom:Â docker psÂ shows container as â€œUpâ€, but you canâ€™t reach the app from host.\n3.1 Check internal port and binding\nFirst, exec into the container:\nbash\ndocker exec -it &lt;container&gt; sh\nInside:\n\nCheck if the app is listening on the expected port:\nbash\nnetstat -tulnp  # or ss -tulnp\nConfirm address:\n\nIf bound toÂ 0.0.0.0:8080, itâ€™s reachable from anywhere on that namespace.\nIf bound toÂ 127.0.0.1:8080Â inside the container, itâ€™s reachable only from inside the container, which often breaks access through the Docker bridge.\n\n\n\nFix in app config:\n\nBind your server toÂ 0.0.0.0Â inside the container (notÂ localhost).\n\n3.2 Check port publishing on the host\nOn the host:\nbash\ndocker ps\nLook at theÂ PORTSÂ column, e.g.:\n\n0.0.0.0:8080-&gt;8080/tcpÂ â€“ mapped correctly.\nEmpty or justÂ 8080/tcpÂ â€“ no host port published.\n\nIf thereâ€™s no mapping, you probably forgotÂ -p:\nbash\ndocker run -d --name api -p 8080:8080 myorg/api:1.0.0\n3.3 Host firewall and reachability\nIf mapping is correct but you still canâ€™t reach:\n\n\nCurl from the host:\nbash\ncurl -v http://localhost:8080/health\n\n\nIf that fails, check:\n\nHost firewall rules (iptables, ufw, firewalld, cloud security groups).\nDocker Desktop/WSL routing on Windows/Mac if applicable.\n\n\n\nIf curl on host fails but curlÂ inside containerÂ works:\n\nApp is healthy internally; issue is port mapping or firewall.\n\n\n\n\n4. App canâ€™t reach another container (DB, cache, etc.)\nSymptom: API reports â€œcanâ€™t connect to DB/Redis/serviceâ€ even though both containers are running.\n4.1 Verify same network\nInspect containers:\ndocker inspect api | grep -A3 Networks\ndocker inspect db  | grep -A3 Networks\n\nCheck they share a common network (e.g.,Â app-netÂ or Composeâ€™s default).\nIf not:\n\nAttach them to the same userâ€‘defined network:\n\ndocker network create app-net\ndocker network connect app-net api\ndocker network connect app-net db\n\n4.2 Check DNS name resolution\nInside the caller container (e.g.,Â api):\ndocker exec -it api sh\nping db\n\n\nIf you get â€œunknown hostâ€: the nameÂ dbÂ doesnâ€™t resolve â†’ wrong network or typo.\nOn userâ€‘defined bridge networks, service/container names become DNS names automatically.\n\nIn Compose, the name used underÂ services:Â is the DNS name (e.g.,Â db).\n4.3 Curl/ping the dependency\nStill insideÂ api:\napk add --no-cache curl  # if minimal image\ncurl -v http://db:5432   # or relevant port/protocol\n\nInterpretation:\n\nConnection refused:\n\nDB is not listening on the advertised port.\nDB is still starting; check DB logs.\n\n\nConnection timeout:\n\nNetwork misconfig, firewall, or wrong hostname.\n\n\nWorks, but app still fails:\n\nApp may be using wrong env vars or URL; verify its connection string.\n\n\n\n\n5. Performance and resource issues\nSymptom: container is â€œrunningâ€ but slow, unresponsive, or periodically crashes.\n5.1 Check live resource usage\nOn the host:\ndocker stats\nSee:\n\nCPU% per container.\nMem usage / limit.\nNetwork I/O.\n\nIf one container is pegging CPU or hitting memory limits, thatâ€™s your suspect.\n5.2 OOM kills and exit code 137\nIf containers mysteriously exit with statusÂ 137, it usually means:\n\nProcess was killed by theÂ OOM killerÂ (out of memory).\nExit code 137 = 128 + 9 (SIGKILL) â€“ typical for OOM conditions.\n\nCheck:\ndocker inspect &lt;container&gt; --format &#039;{{.State.ExitCode}} {{.State.OOMKilled}}&#039;\nIfÂ OOMKilled=true:\n\nIncrease memory limit:\ndocker run -d --memory=&quot;1g&quot; ...\nOr reduce app memory usage (heap size, caches, concurrency).\n\n5.3 Debugging inside the container\nExec into the container and use familiar tools:\ndocker exec -it api sh\ntop            # or htop if installed\nps aux         # see processes\n\nCheck if:\n\nThere are too many threads/connectors.\nSome background process is hogging CPU.\nLogs show frequent GC or memory issues (for JVM apps).\n\n\n6. Building a personal â€œdebug checklistâ€\nTo avoid flailing, encode this playbook as yourÂ SOPÂ (Standard Operating Procedure). For any broken container:\n\n\nIs the container running?\n\ndocker ps -aÂ â†’ status and exit code.\nIf not running:\n\ndocker logs &lt;container&gt;\ndocker inspect &lt;container&gt; ...State.*\n\n\n\n\n\nIf running but not reachable from host:\n\nExec in:Â docker exec -it &lt;container&gt; sh\nCheck app listening and bind address (0.0.0.0 vs 127.0.0.1).\nCheckÂ docker psÂ for correctÂ PORTSÂ mapping.\nTest withÂ curlÂ from host and inside.\n\n\n\nIf app canâ€™t reach another container:\n\nConfirm same network (docker network inspect).\nUse service name as hostname (e.g.,Â db).\nExec into caller;Â ping/curlÂ to dependency.\n\n\n\nIf performance issues or crashes:\n\ndocker statsÂ for CPU/memory.\nLook for OOMKilled, exit code 137.\nUseÂ top,Â psÂ inside container to see noisy processes.\n\n\n\nIf still stuck:\n\nReproduce with a minimal image (e.g., alpine + curl) to isolate network/dns issues.\nSimplify Dockerfile/Compose config temporarily to narrow down the culprit.\n\n\n\nMapping to Kubernetes later\nIn Kubernetes, youâ€™ll do theÂ same steps, just withÂ kubectl:\n\ndocker psÂ â†’Â kubectl get pods.\ndocker logsÂ â†’Â kubectl logs.\ndocker execÂ â†’Â kubectl exec.\ndocker inspectÂ â†’Â kubectl describe pod.\ndocker statsÂ â†’ metrics viaÂ kubectl topÂ or monitoring stack.\n\nSo every debugging reflex you build with Docker translates almost 1:1 to Pods and Services in k8s, just with different commands."},"1-Devops/Docker-Mastery/11-from-docker-to-orchestrators":{"slug":"1-Devops/Docker-Mastery/11-from-docker-to-orchestrators","filePath":"1-Devops/Docker-Mastery/11-from-docker-to-orchestrators.md","title":"11. From Docker to Orchestrators","links":[],"tags":[],"content":"All the mental models youâ€™ve built (images, containers, networks, volumes, Compose) are the foundation of Swarm and Kubernetes. Orchestrators donâ€™t replace Docker; they automate it atÂ cluster scale: scheduling, selfâ€‘healing, service discovery, and rolling upgrades.\n\n1. Why Docker alone isnâ€™t enough in production\nDocker on a single host is great, but production needs more:\n\nScheduling\n\nDecideÂ whichÂ node runs each container.\nRebalance when nodes join/leave or fail.\n\n\nSelfâ€‘healing\n\nRestart crashed containers automatically.\nReschedule them to another node if the current node dies.\n\n\nScaling\n\nRun N replicas of a service across multiple nodes.\nScale up/down based on traffic or SLOs.\n\n\nService discovery &amp; load balancing\n\nGive clients a stable name (e.g.,Â orders-api) even as containers move around.\nLoad balance across replicas.\n\n\n\nWhere orchestrators fit:\n\nDocker Swarm\n\nDockerâ€™s builtâ€‘in clustering/orchestration mode.\nSimpler mental model, tight Docker integration.\n\n\nKubernetes\n\nDeâ€‘facto standard orchestrator.\nRich API, strong ecosystem, steeper learning curve.\n\n\n\nYour Docker concepts (images, containers, networks, volumes, Compose) become theÂ building blocksÂ that Swarm and Kubernetes manage across many machines.\n\n2. Concept mapping table\nHereâ€™s how your Docker knowledge maps into Kubernetes (and, conceptually, Swarm):\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDocker / Compose conceptKubernetes conceptMental mappingImageImage (same)Same artifact; pulled by nodes, defined in Pod specsContainerContainer inside aÂ PodPod = 1+ tightly coupled containers (usually 1 for your apps)docker runPod (direct) / Deployment (managed Pods)Deployment continuously ensures Pods existCompose serviceDeployment + ServiceDeployment: replicas; Service: stable DNS + load balancingDocker networkCluster network + Service + CNICNI provides Pod IPs; Services provide stable virtual IPs/DNSContainer name DNSService name / Pod DNSdbÂ in Compose â†’Â dbÂ Service in k8sDocker volumePersistentVolume (PV)Actual storage definitionVolume declaration in YAMLPersistentVolumeClaim (PVC)Pod requests storage via PVCBind mounthostPath volumeDirect host path mounting (used sparingly in k8s)Compose stack (file)Manifests: Deployment, Service, PVC, etc.Same intent, more detailed resourcesdocker logsÂ /Â execkubectl logsÂ /Â kubectl execSame debugging pattern, different CLI\nIf youâ€™re comfortable with:\n\nImages and containers.\nNetworks and DNS service names.\nVolumes and data separation.\nCompose files as multiâ€‘service specs.\n\nâ€¦then you already understand 70% of what Kubernetes objects are trying to expressâ€”just in a more explicit way.\n\n3. Swarm in one page (optional, but good for intuition)\nSwarm mode turns Docker into a simple orchestrator.\nBasic concepts\n\nNodeÂ â€“ a Docker Engine participating in the swarm (manager or worker).\nServiceÂ â€“ a declarative description of a set of tasks (containers) with image, replicas, ports.\nTaskÂ â€“ one running container which is part of a service.\nOverlay networksÂ â€“ multiâ€‘host networks for interâ€‘service communication.\n\nCommon commands\nInitialize Swarm (on manager):\ndocker swarm init\nCreate a service:\ndocker service create \\\n  --name web \\\n  --replicas 3 \\\n  -p 80:80 \\\n  nginx:1.25-alpine\n\nScale:\ndocker service scale web=5\nInspect:\ndocker service ls\ndocker service ps web\n\nTakeaway:\n\nAÂ Swarm serviceÂ is to containers whatÂ docker-composeÂ service is to single hostâ€”but with scheduling and replication across nodes.\nItâ€™s simpler than Kubernetes and uses the same Docker CLI mental model, so itâ€™s good training for thinking about clusters.\n\n\n4. Kubernetesâ€‘oriented view\nKubernetes is more verbose, but itâ€™s still doing â€œDocker plus orchestration,â€ just with a very rich API.\nWhy images, tags, and security hardening matter more\nIn Kubernetes:\n\nYour images are pulled ontoÂ many nodes.\nAny mistake in the image (secrets baked in, running as root, huge size) is multiplied across the cluster.\nRolling updates, autoscaling, and chaos testing all assume images are:\n\nSmall (fast to pull).\nCorrectly tagged (so you know whatâ€™s running).\nReasonably secure (nonâ€‘root, minimal base).\n\n\n\nEverything you did right in Docker:\n\nMultiâ€‘stage builds.\nNonâ€‘rootÂ USER.\nStable tagging (1.2.3,Â git-abc1234).\nHEALTHCHECK endpoints.\n\nâ€¦makes Kubernetes deployments smoother:\n\nRolling updates can quickly pull new images.\nReadiness/liveness probes call your health endpoints.\nRBAC + PodSecurity settings play nicer with nonâ€‘root images.\n\nHow Dockerfile/image choices affect rolling updates and autoscaling\n\nRolling updates\n\nKubernetes pulls the new image on each node and starts new Pods gradually.\nLarge images â†’ slow rollouts â†’ longer partial deployments.\nMisconfigured health endpoints â†’ Pods marked not ready â†’ failed updates.\n\n\nAutoscaling\n\nHPA (Horizontal Pod Autoscaler) scales Pod count based on CPU, memory, or custom metrics.\nIf your image is CPUâ€‘heavy due to debug tools or unbounded resource usage, autoscaling decisions are noisier.\nResource requests/limits must match realistic container behavior (the sameÂ --cpus,Â --memoryÂ you practiced in Docker).\n\n\n\nIn short: good Docker hygiene is a prerequisite for sane Kubernetes behavior.\n\n5. How to practice: from Compose to k8s\nThe best way to internalize the mapping is toÂ port a real Compose stackÂ to Kubernetes step by step.\nAssume you have a simpleÂ docker-compose.yml:\nversion: &quot;3.9&quot;\n\nservices:\n  db:\n    image: postgres:15-alpine\n    environment:\n      POSTGRES_DB: orders\n      POSTGRES_USER: orders_user\n      POSTGRES_PASSWORD: secret\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n\n  api:\n    image: myorg/orders-api:1.0.0\n    ports:\n      - &quot;8080:8080&quot;\n    environment:\n      DB_HOST: db\n      DB_PORT: 5432\n      DB_NAME: orders\n      DB_USER: orders_user\n      DB_PASSWORD: secret\n    depends_on:\n      - db\n\nvolumes:\n  pgdata:\n\n\nStepâ€‘wise migration outline\nStep 1 â€“ API Deployment + Service\n\nCreate a Deployment forÂ api:\n\nspec.template.spec.containers[0].image = myorg/orders-api:1.0.0.\nEnv vars copied intoÂ env:Â section.\n\n\nCreate a Service forÂ api:\n\nTypeÂ NodePortÂ orÂ LoadBalancerÂ for external access to 8080.\nPort 8080 mapped to container port 8080.\n\n\n\nStep 2 â€“ DB Deployment/StatefulSet + Service\n\nCreate a Deployment or StatefulSet forÂ db:\n\nimage: postgres:15-alpine.\nEnv vars for DB credentials.\n\n\nCreate a PVC and Volume for data:\n\nPersistentVolumeClaimÂ representing theÂ pgdataÂ volume.\nMount atÂ /var/lib/postgresql/data.\n\n\nCreate a ClusterIP ServiceÂ db:\n\nPort 5432.\nDNS name:Â dbÂ in the namespace.\n\n\n\nStep 3 â€“ Wire API to DB\n\nIn the API Deployment, set:\n\nenv:\n  - name: DB_HOST\n    value: db\n  - name: DB_PORT\n    value: &quot;5432&quot;\n\n\nNowÂ apiÂ Pods use the Service nameÂ dbÂ (just like Compose usesÂ dbÂ as service name).\n\nStep 4 â€“ Add health probes\n\nTranslate your Docker HEALTHCHECK to k8s probes:\n\nreadinessProbe:\n  httpGet:\n    path: /actuator/health\n    port: 8080\n  initialDelaySeconds: 10\n  periodSeconds: 10\n\nlivenessProbe:\n  httpGet:\n    path: /actuator/health\n    port: 8080\n  initialDelaySeconds: 30\n  periodSeconds: 30\n\n\nStep 5 â€“ Apply resource limits\n\nUse your Docker experience withÂ --cpus,Â --memoryÂ to choose:\n\nresources:\n  requests:\n    cpu: &quot;250m&quot;\n    memory: &quot;256Mi&quot;\n  limits:\n    cpu: &quot;1&quot;\n    memory: &quot;512Mi&quot;\n\n\nThis stepwise port keeps you focused onÂ conceptual equivalence, not memorizing YAML.\n\nHow to build intuition, not just YAML muscle memory\n\nStart every k8s object by asking:\n\nâ€œWhat is this in Docker/Compose terms?â€\nâ€œWhich part of my Compose file is this mapping?â€\n\n\nPractice debugging in k8s with the same mental sequence:\n\nkubectl get podsÂ (likeÂ docker ps).\nkubectl logsÂ (likeÂ docker logs).\nkubectl execÂ (likeÂ docker exec).\nkubectl describeÂ (similar toÂ docker inspectÂ + events).\n\n\n\nYour Dockerâ€‘level understanding (images, containers, networking, volumes, Compose, debugging) is not wasted; itâ€™s theÂ core foundationÂ that makes Kubernetes feel like a natural next layer instead of a completely foreign system."},"1-Devops/Docker-Mastery/2-docker-containers":{"slug":"1-Devops/Docker-Mastery/2-docker-containers","filePath":"1-Devops/Docker-Mastery/2-docker-containers.md","title":"2. Docker Containers","links":[],"tags":[],"content":"Most people start by thinking of containers as â€œlightweight VMs.â€ That mental model works for a few days, then hurts you for years. A container isÂ just a processÂ (or a few) with isolation and a custom filesystem view, built from an image. Once you see it that way, a lot of â€œDocker magicâ€ becomes predictable.\nThis article walks through container internals, lifecycle, core commands, resource limits, and a practical troubleshooting mindset.\n\n1. Mental Model: What a Container Really Is\nUnder the hood, a container is:\n\nAÂ Linux processÂ (PID on the host)\nRunning with:\n\nIts own filesystem view (from an image + writable layer)\nIts own network namespace (own IP stack)\nIts own PID namespace (process tree inside container)\nResource limits enforced byÂ cgroups\n\n\n\nKey idea:\nIf the container is â€œrunning,â€ that means there is aÂ processÂ running on the host. If the process exits, the container stops. Thereâ€™s no â€œguest OSâ€ like a VM.\nYou can prove this to yourself:\n# Run a simple container\ndocker run --name demo -d alpine:3.19 sleep 1000\n\n# On the host, find the PID\ndocker top demo\n# or\nps aux | grep sleep\n\n\nYouâ€™ll see theÂ sleepÂ process; thatâ€™s your container.\n\n2. Container Lifecycle: From Image to Running Process\nFor a single container, the lifecycle looks like this:\n\nCreate\nDocker allocates metadata, filesystem, network namespace, etc.\n(You can do this explicitly withÂ docker create, butÂ docker runÂ does it implicitly.)\nStart\nDocker launches the containerâ€™s main process (the ENTRYPOINT+CMD from the image).\nRunning\nThe process is alive. Docker tracks its stdout/stderr, resources, and exit code.\nStopped\nThe process exits. The container object + writable layer are still present on disk.\nRemoved\nDocker deletes the containerâ€™s metadata and filesystem layer.\n\nThe main command you use:\ndocker run\n\nis a convenience wrapper for:\n\ndocker create\ndocker start\n\nUnderstanding that helps when you debug:\n\nA container that â€œexits immediatelyâ€ is just a process that finishes quickly.\nTo keep it running, you need a longâ€‘running process (server, tail, sleep, etc.).\n\n\n3. Creating and Running Containers: Core Patterns\n3.1 Oneâ€‘off interactive containers\nUse these whenever you want a temporary shell:\nbash\ndocker run --rm -it alpine:3.19 sh\nFlags breakdown:\n\n--rmÂ â†’ delete container when it stops.\n-itÂ â†’ interactive TTY (so you get a shell).\n\nMental model: this is your â€œdisposable debuggerâ€ or â€œscratch VM,â€ but itâ€™s still just a process.\n\n3.2 Longâ€‘running services\nTypical pattern for APIs, web servers, etc.:\nbash\ndocker run -d --name web -p 8080:80 nginx:1.25-alpine\n\n-dÂ â†’ detached mode (run in background).\n--name webÂ â†’ stable name for logs, exec, etc.\n-p 8080:80Â â†’ map host port 8080 to container port 80.\n\nThis container:\n\nIs backed by a main process (nginxÂ master process).\nWill stop if that process crashes or exits.\n\nIf it keeps dying, donâ€™t think â€œVM crashedâ€; think â€œprocess crashedâ€ and check logs.\n\n3.3 Environment variables and configuration\nPass configuration at runtime:\ndocker run -d --name orders-api \\\n  -p 8080:8080 \\\n  -e SPRING_PROFILES_ACTIVE=prod \\\n  -e DB_HOST=db \\\n  myorg/orders-api:1.0.0\n\n\nThis becomes:\n\nEnvironment variables visible to the process inside the container.\nExactly likeÂ exportÂ on a normal Linux host.\n\nThis is usually the right place for nonâ€‘secret configuration (URLs, flags, modes). Secrets should be handled more carefully in real systems (secret managers, etc.).\n\n4. Managing Containers Dayâ€‘toâ€‘Day\nThink of these as your daily driver commands.\n4.1 Listing containers\ndocker ps        # running only\ndocker ps -a     # all containers (running + stopped)\n\n\nUseful columns:\n\nNAMES â†’ what to use withÂ logs,Â exec,Â inspect.\nSTATUS â†’ â€œUp 5 minutesâ€, â€œExited (0) 2 seconds agoâ€.\n\n4.2 Stopping, starting, removing\ndocker stop web     # send SIGTERM, wait, then SIGKILL after timeout\ndocker start web    # restart a stopped container\ndocker rm web       # remove container (must be stopped)\n\nIf you want to kill and recreate:\ndocker rm -f web    # force remove (stop + rm)\n\nTypical dev loop:\n\ndocker rm -f web\ndocker build -t myorg/web:dev .\ndocker run ...\n\n\n4.3 Logging: stdout and stderr as your primary log sink\nDocker automatically captures the main processâ€™s stdout and stderr:\ndocker logs web          # historical logs\ndocker logs -f web       # follow logs (tail -f)\n\nGood practice:\n\nYour app should log to stdout/stderr (not to local files inside the container).\nThat way, orchestrators (Compose, Swarm, Kubernetes, log collectors) can pick up logs easily.\n\nFor a Spring Boot service:\n\nConfigure logs to go to console â†’ Docker (and later Kubernetes) can aggregate them.\n\n\n4.4 Exec into containers\nWhen something is weird, â€œenterâ€ the container:\ndocker exec -it web sh\n# or for Debian/Ubuntu based images:\ndocker exec -it web bash\n\n\nUse this to:\n\nInspect filesystem.\nRun curl, ping, or appâ€‘specific debug commands.\nQuickly check config files and environment vars (env).\n\nItâ€™s equivalent to SSHing into a VM, but youâ€™re really just attaching to a processâ€™s namespace.\n\n5. Resources: Making Sure Containers Donâ€™t Eat the Host\nBecause containers share the host kernel, they can starve each other if you donâ€™t set limits.\n5.1 CPU limits\nbash\ndocker run -d --name cpu-demo --cpus=&quot;1.0&quot; myorg/task:1.0.0\nThis roughly constrains the container to 1 CPU core worth of time. Without limits, one container can saturate the host CPU, especially on dev machines.\n5.2 Memory limits\nbash\ndocker run -d --name mem-demo --memory=&quot;512m&quot; myorg/task:1.0.0\n\nIf the process allocates more than that, the kernel may kill it with OOM (Out Of Memory).\nYouâ€™ll see exit code 137 (killed) or similar in Docker.\n\nCombine:\ndocker run -d --name api \\\n  -p 8080:8080 \\\n  --cpus=&quot;1&quot; \\\n  --memory=&quot;512m&quot; \\\n  myorg/orders-api:1.0.0\n\n\nThis is closer to how youâ€™d run things in production.\n5.3 Checking usage:Â docker stats\ndocker stats\nGives live CPU, memory, network, I/O usage per container.\nIf one service is misbehaving, this is your quick â€œtopâ€ for containers.\n\n6. Restart Policies: Making Containers Survive Crashes\nIn pure Docker (without an orchestrator), restart policies give minimal selfâ€‘healing:\ndocker run -d --name api \\\n  --restart=on-failure \\\n  -p 8080:8080 \\\n  myorg/orders-api:1.0.0\n\n\nCommon policies:\n\nnoÂ (default): never restart automatically.\non-failure: restart only if exit code â‰  0.\nalways: always restart if stopped.\nunless-stopped: restart unless you manually stopped it.\n\nUse cases:\n\non-failureÂ for tasks that might crash but shouldnâ€™t be resurrected if cleanly completed.\nalwaysÂ /Â unless-stoppedÂ for longâ€‘running services on standalone hosts.\n\nLater, in Kubernetes, this concept maps to Pod restart behavior controlled by the controller (Deployment, etc.).\n\n7. Containers vs Images vs Volumes: How Changes Persist\nA common confusion: â€œI edited a file inside the container, but when I recreate it, my changes are gone.â€\nKey rules:\n\nImagesÂ are immutable.\nContainer writable layerÂ is ephemeral:\n\nIf youÂ docker rmÂ the container, changes in that layer vanish.\n\n\nVolumesÂ are persistent:\n\nThey outlive containers and can be attached to new ones.\n\n\n\nWorkflow implication:\n\nToÂ change the app code or binaries, you usually:\n\nChange source.\nRebuild image.\nStart new container from new image.\n\n\nToÂ persist dataÂ (database, uploads):\n\nUse volumes, not the containerâ€™s writable layer.\n\n\n\n\n8. Debugging Containers: A Practical Playbook\nWhen something â€œdoesnâ€™t work,â€ follow a steady sequence.\n8.1 Container exits immediately\n\nCheck status:\nbash\ndocker ps -a\nInspect exit code and logs:\nbash\ndocker logs my-container\n\nCommon causes:\n\nWrong command inÂ CMDÂ /Â ENTRYPOINTÂ (executable not found).\nMain process completes and exits (e.g., script finishing).\nCrash due to missing config/env.\n\nFix: make sure the main process is longâ€‘running and configured correctly.\n\n8.2 Container running, but port not accessible\nChecklist:\n\n\nIs the containerÂ running?\nbash\ndocker ps\n\n\nIs the app actually listening on the right port inside the container?\ndocker exec -it api sh\n# inside container:\nnetstat -tulnp  # or ss -tulnp\nMany apps bind toÂ 127.0.0.1; inside a container, thatâ€™s still the container only.\nYou usually want to bind toÂ 0.0.0.0.\n\n\nIs the port mapped on the host?\ndocker ps # look at PORTS column, e.g. 0.0.0.0:8080-&gt;8080/tcp\n\n\nCan you curl from the host?\ncurl http://localhost:8080/health\n\n\nIf it works inside the container but not outside:\n\nThe app might only listen on localhost inside the container.\nOr the port mapping (-p) is wrong/missing.\n\n\n8.3 Container canâ€™t reach another container (DB, cache, etc.)\nChecklist:\n\nAre they on the same Docker network?\nIs the dependency container running?\nIs your app using theÂ container nameÂ as hostname (on userâ€‘defined networks)?\n\nDebug:\ndocker exec -it api sh\n# inside api container:\nping db\napk add --no-cache curl\ncurl http://db:5432  # or appropriate protocol/port\n\n\nIf DNS name doesnâ€™t resolve, check that both are attached to the same userâ€‘defined network and not using the default bridge incorrectly.\n\n9. Containers in the Bigger Picture: Why This Mental Model Matters\nOnce you internalize:\n\nContainer = process with isolation.\nImage = filesystem + metadata.\nVolume = persistent data.\n\nthen:\n\nDebugging Docker is just debugging Linux processes with extra tooling.\nMoving to Kubernetes is easier because Pods are also just wrapper abstractions around containers/processes.\nYou stop expecting â€œVMâ€‘likeâ€ behaviors (like â€œI changed a file and it should persist foreverâ€) and design images + volumes properly.\n"},"1-Devops/Docker-Mastery/3-docker-networking":{"slug":"1-Devops/Docker-Mastery/3-docker-networking","filePath":"1-Devops/Docker-Mastery/3-docker-networking.md","title":"3. Docker Networking","links":[],"tags":[],"content":"Docker networking feels magical until something doesnâ€™t connectâ€”then it feels like black magic. The cure is a clear mental model: each container has itsÂ own network stack, and Docker wires these stacks together using Linux primitives (bridges, veth pairs, iptables, etc.). Once you understand that, serviceâ€‘toâ€‘service communication becomes predictable instead of trialâ€‘andâ€‘error.\nThis article builds that model, then walks through real commands and a practical debugging playbook.\n\n1. Mental Model: What Happens When YouÂ docker run -p 8080:80 nginx\nLetâ€™s start with the most common confusion: why doesÂ localhostÂ sometimes work and sometimes not?\nWhen you run:\ndocker run -d --name web -p 8080:80 nginx:1.25-alpine\nDocker does roughly this under the hood:\n\nCreates aÂ network namespaceÂ for the container (its own IP stack).\nCreates aÂ veth pair: one end inside the container, one end on the host bridge.\nConnects the host end to a Linux bridge (oftenÂ docker0) that acts like a virtual switch.\nAssigns the container an IP on that bridge (e.g.Â 172.17.0.2).\nSets upÂ NAT rulesÂ so traffic hitting hostÂ :8080Â gets DNATâ€‘ed to containerÂ 172.17.0.2:80.\n\nKey consequences:\n\nInside the container,Â localhostÂ means â€œthis container,â€ not your host.\nFrom the host, you normally reach containers viaÂ published portsÂ (-p) or the container IP (bridge network).\nContainers talk to each other directly using their IPs or (on userâ€‘defined networks) theirÂ service names.\n\n\n2. Builtâ€‘in Network Drivers: bridge, host, none, overlay\nDocker ships with several network â€œdrivers.â€ You rarely need all of them, but knowing what they do matters.\n2.1Â bridgeÂ â€“ the default\n\nDefault for containers when you donâ€™t specifyÂ --network.\nSingleâ€‘host, NATâ€‘based networking.\nContainers get IPs likeÂ 172.17.x.xÂ and talk to each other using those IPs.\n\nFor quick experiments, this is fine, but for anything nonâ€‘trivial youâ€™ll preferÂ userâ€‘defined bridgesÂ (stillÂ bridgeÂ driver, but created by you).\n2.2 Userâ€‘defined bridge â€“Â the recommended default\ndocker network create app-net\nProperties:\n\nContainers attached to this network get:\n\nTheir own IP in that network.\nBuiltâ€‘inÂ DNS: container names â†’ IPs.\n\n\nDocker does some extra isolation and better defaults than the implicitÂ bridge.\n\nTypical workflow:\ndocker run -d --name db --network app-net \\\n  -e POSTGRES_PASSWORD=secret \\\n  postgres:15-alpine\n\ndocker run -d --name api --network app-net \\\n  -e DB_HOST=db \\\n  myorg/orders-api:1.0.0\n\n\nNow:\n\napiÂ can reachÂ dbÂ at hostnameÂ db:5432.\nYou donâ€™t care what the actual IPs are (Docker DNS resolves names).\n\n2.3Â hostÂ â€“ share host network\ndocker run --net=host nginx:1.25-alpine\n\nContainer shares the hostâ€™s network namespace.\nNo port mapping needed (or possible): if Nginx listens onÂ :80Â in container, itâ€™s listening on hostÂ :80.\n\nPros:\n\nSlightly lower overhead, easier for some tools (like sniffers, some monitoring agents).\n\nCons:\n\nNo isolation.\nPort conflicts with host processes.\nNot available on Docker Desktop in the same way for all OSes.\n\nUse it sparingly, mostly for special systemâ€‘level stuff.\n2.4Â noneÂ â€“ fully isolated\ndocker run --network none alpine:3.19\n\nContainer has no network connectivity.\nOnly useful for specialized isolation scenarios or when you explicitly donâ€™t want any network.\n\n2.5Â overlayÂ â€“ multiâ€‘host networks\n\nUsed with Docker Swarm (or similar setups).\nCreates a logical network spanning multiple hosts, often via VXLAN.\nLets services running on different nodes talk as if on the same LAN.\n\nFor now, just remember: overlay is for clustering; youâ€™re unlikely to need it in simple, singleâ€‘host dev setups.\n\n3. Ports: Inside vs Outside, and WhyÂ localhostÂ Lies\nEvery networked app has two sides:\n\nInside the container: the port the process binds to.\nOutsideÂ (host or other machines): how you expose that port.\n\nExample:\ndocker run -d --name api -p 8080:8080 myorg/orders-api:1.0.0\n\nInside container: your app binds toÂ 8080.\nOn host: Docker publishes hostÂ 0.0.0.0:8080Â â†’ containerÂ &lt;container-ip&gt;:8080.\n\nCommon traps:\n\nApp binds toÂ 127.0.0.1Â inside the container:\n\nItâ€™s reachableÂ from insideÂ the container, but not from the host through bridged networking.\nYou usually want it to bind toÂ 0.0.0.0Â inside the container.\n\n\nForgotÂ -pÂ entirely:\n\nContainer can still serve other containers on same network.\nBut hostÂ curl http://localhost:8080Â fails, because nothing is listening on hostÂ 8080.\n\n\n\nQuick check:\ndocker ps # Look at PORTS column: should show something like 0.0.0.0:8080-&gt;8080/tcp\n\n4. Docker Networking in Practice: Multiâ€‘Container Stack\nLetâ€™s wire up a simple but realistic stack:Â apiÂ +Â db.\n4.1 Create a network\ndocker network create app-net\n4.2 Run the database\ndocker run -d --name db --network app-net \\\n  -e POSTGRES_PASSWORD=secret \\\n  -v pgdata:/var/lib/postgresql/data \\\n  postgres:15-alpine\n\nObservations:\n\ndbÂ gets IP onÂ app-netÂ (sayÂ 172.18.0.2).\nDocker DNS will resolveÂ dbÂ to that IP for containers onÂ app-net.\n\n4.3 Run the API\ndocker run -d --name api --network app-net \\\n  -p 8080:8080 \\\n  -e DB_HOST=db \\\n  -e DB_PORT=5432 \\\n  myorg/orders-api:1.0.0\n\n\nInside the container, your Spring Boot app will connect to Postgres at hostÂ db, portÂ 5432.\nYou can now:\n\nFrom host:Â curl http://localhost:8080/actuator/health\nFromÂ apiÂ container:Â ping dbÂ orÂ nc -z db 5432\n\n\n5. Inspecting Networks: Seeing the Wiring\nTo see whatâ€™s really going on, inspect:\ndocker network inspect app-net\nYouâ€™ll get:\n\nSubnet and gateway for the network (e.g.Â 172.18.0.0/16).\nList of connected containers and their IP addresses.\nDriver (bridge) and some config.\n\nThis is yourÂ source of truthÂ when youâ€™re debugging connectivity:\n\nIsÂ apiÂ actually onÂ app-net?\nIsÂ dbÂ actually onÂ app-net?\nAre there multiple networks with similar names?\n\n\n6. Debugging Connectivity: A Stepâ€‘Byâ€‘Step Playbook\nWhen â€œservice A canâ€™t talk to service B,â€ donâ€™t guess. Follow a consistent sequence.\n6.1 Check containers and networks\n\n\nAre both containers running?\nbash\ndocker ps\n\n\nAre they on theÂ same network?\nbash\n\n\ndocker inspect api | grep -A3 Networks\ndocker inspect db  | grep -A3 Networks\n\nIf theyâ€™re not on the same userâ€‘defined network, Docker DNS wonâ€™t resolve names across them.\n\n6.2 Exec into the caller and test\nFrom theÂ apiÂ container:\ndocker exec -it api sh\n\n# Inside api:\nping db           # should resolve and respond (if ping is installed)\napk add --no-cache curl\ncurl -v http://db:5432   # or use nc/telnet if appropriate\n\n\nInterpretation:\n\nIfÂ ping dbÂ fails with â€œunknown host,â€ DNS is broken (wrong network, typo in name).\nIf DNS works butÂ curlÂ orÂ ncÂ fails:\n\ndbÂ might not be listening on the expected port.\nFirewall or misconfig insideÂ dbÂ container.\n\n\n\n\n6.3 Host â†” container access\nIf host canâ€™t reach container:\n\n\nConfirm port mapping:\n`\n\n\ndocker ps\n# Check PORTS: 0.0.0.0:8080-&gt;8080/tcp?\n\n`\n\n\nCurl from host:\ncurl -v http://localhost:8080/health\n\n\nIf that fails, curl from inside container:\ndocker exec -it api sh curl -v http://localhost:8080/health\n\n\n\nWorks inside but not from host â†’ usually port mapping or firewall issue.\nDoesnâ€™t even work inside â†’ app not listening on expected port or binding to wrong interface.\n\n\n7. Host Networking: When and Why (Not) to Use It\n--net=hostÂ (orÂ --network host) gives containers the hostâ€™s network stack.\nPros:\n\nNo NAT overhead.\nUseful for network tools that need direct host access (e.g., sniffers, some monitoring agents).\nSimplifies some local dev scenarios (noÂ -pÂ needed).\n\nCons:\n\nNo port isolation; a container can bind host ports directly and conflict with host services.\nLess separation in terms of firewalling and security.\n\nExample:\ndocker run --net=host --name monitor \\   some/network-monitoring-tool\nFor most application containers, especially in multiâ€‘tenant or production settings, stick with bridged networks and explicitÂ -pÂ mappings.\n\n8. From Docker Networking to Kubernetes Services\nYouâ€™re ultimately heading to Kubernetes, so itâ€™s useful to see how this mental model transfers:\n\nDockerÂ userâ€‘defined networkÂ â†’ KubernetesÂ cluster networkÂ (via CNI plugin).\nDockerÂ container name DNSÂ â†’ KubernetesÂ Service nameÂ andÂ Pod DNS.\nDockerÂ -p host:containerÂ â†’ KubernetesÂ Service type NodePort / LoadBalancer / Ingress.\n\nIf youâ€™re comfortable with:\n\nCreating networks,\nAttaching containers,\nUsing DNS names instead of IPs,\nDebugging usingÂ exec,Â curl, andÂ network inspect,\n\nthen Kubernetes networking will feel like a structured extension, not a totally new universe.\n\n9. Practical Rules of Thumb\nTo anchor all this, here are some simple rules you can treat as defaults:\n\nUseÂ userâ€‘defined bridge networksÂ for any multiâ€‘container app, not the defaultÂ bridge.\nAlways think: â€œinside vs outsideâ€ ports; donâ€™t trustÂ localhostÂ without context.\nPreferÂ DNS namesÂ (container names) over hardcoded IPs for containerâ€‘toâ€‘container calls.\nBuild a routine for debugging:\n\ndocker ps\ndocker network ls\ndocker network inspect\ndocker logs\ndocker exec + curl/ping\n\n\n\nThis turns â€œDocker networking is magicâ€ into â€œDocker networking is just Linux networking with nicer defaults.â€\n"},"1-Devops/Docker-Mastery/4-docker-volumes":{"slug":"1-Devops/Docker-Mastery/4-docker-volumes","filePath":"1-Devops/Docker-Mastery/4-docker-volumes.md","title":"4. Docker Volumes","links":[],"tags":[],"content":"Containers are designed to be disposable. Thatâ€™s perfect for stateless services, but terrible for things like databases, uploads, and logs.Â VolumesÂ are how Docker decouples the lifecycle of your data from the lifecycle of your containers.\nThis article builds a clear mental model of volumes, compares them with bind mounts, and gives you practical patterns and a debugging playbook.\n\n1. Mental Model: Container FS vs Volume\nInside every container you have:\n\nAÂ readâ€‘onlyÂ image filesystem (from the image layers).\nAÂ writableÂ layer on top (containerâ€™s own changes).\n\nWhen youÂ docker rmÂ a container, that writable layer (and all its changes) disappears. Thatâ€™s why:\n\nWriting a Postgres data directory toÂ /var/lib/postgresql/dataÂ inside the container without a volume means your database disappears when the container is deleted.\nEditing app code inside the container is a temporary hack; it vanishes on rebuild.\n\nVolumes solve this by creatingÂ separate, managed storageÂ that:\n\nLives outside the containerâ€™s writable layer.\nCan be attached to any container.\nSurvives container deletion.\n\nSo think:\n\nContainer filesystem = ephemeral.\nVolumeÂ = persistent, containerâ€‘independent data.\n\n\n2. Types of Storage: Named Volumes vs Bind Mounts vs Tmpfs\nDocker supports three main storage concepts:\n\n\nNamed volumes\n\nManaged by Docker.\nIdentified by name (pgdata,Â redis-data).\nStored under Dockerâ€™s data directory (path differs by OS).\nBest choice for persistent data in most cases.\n\n\n\nBind mounts\n\nDirect mapping of a host path into the container (/home/user/app:/app).\nGreat for development (live code reload), or when you explicitly want to use a host directory.\nYou manage the directory; Docker just mounts it.\n\n\n\nTmpfs mounts\n\nInâ€‘memory filesystem for ephemeral data.\nContents are never written to disk on the host.\nUseful for sensitive or highâ€‘churn temporary data.\n\n\n\nHighâ€‘level rule:\n\nUseÂ named volumesÂ for app data (databases, queues).\nUseÂ bind mountsÂ for development workflows and special host integrations.\nUseÂ tmpfsÂ for sensitive temp data or when you really want inâ€‘memory only.\n\n\n3. Named Volumes: Your Default for Persistent Data\n3.1 Creating and listing named volumes\ndocker volume create pgdata\ndocker volume ls\ndocker volume inspect pgdata\n\n\nInspect shows:\n\nName.\nDriver (usuallyÂ local).\nMountpoint on the host (where Docker stores the data).\n\nYou typically donâ€™t touch that path directly; you let Docker manage it.\n3.2 Using a named volume with a database\nPostgres example:\ndocker run -d --name db \\\n  -e POSTGRES_PASSWORD=secret \\\n  -v pgdata:/var/lib/postgresql/data \\\n  postgres:15-alpine\n\n\nHere:\n\npgdataÂ is theÂ named volume.\nInside the container, Postgres writes toÂ /var/lib/postgresql/data.\nOn the host, Docker maps that to some internal path forÂ pgdata.\n\nNow try:\n\n\nInsert some data into the DB.\n\n\nDestroy the container:\nbash\ndocker rm -f db\n\n\nStart a new container with the same volume:\n\n\ndocker run -d --name db2 \\\n  -e POSTGRES_PASSWORD=secret \\\n  -v pgdata:/var/lib/postgresql/data \\\n  postgres:15-alpine\n\nYour data is still there. The container is disposable; theÂ volumeÂ is the durable component.\n3.3 Sharing volumes between containers\nYou can attach the same volume to multiple containers:\ndocker run -d --name db-admin \\\n  --network app-net \\\n  -v pgdata:/var/lib/postgresql/data:ro \\\n  some/pgadmin-image\n\n\n\n:roÂ makes it readâ€‘only for that container.\nBoth DB and admin UI share the same underlying data directory.\n\nThis pattern is powerful but be careful:\n\nTwo writers to the same data directory â†’ potential corruption unless the app explicitly supports it.\n\n\n4. Bind Mounts: Perfect for Development\nBind mounts map a host directory directly into the container.\n4.1 Basic example: mounting source code\ndocker run -d --name web-dev \\\n  -p 3000:3000 \\\n  -v &quot;$PWD/src:/usr/src/app&quot; \\\n  node:22-alpine \\\n  sh -c &quot;cd /usr/src/app &amp;&amp; npm install &amp;&amp; npm run dev&quot;\n\n\nHere:\n\nHostÂ $PWD/srcÂ is mounted into containerÂ /usr/src/app.\nWhen you edit source files on the host, changes are instantly visible inside the container.\nPerfect for hotâ€‘reloading dev servers (Node, React, Angular, etc.).\n\n4.2 Pros and cons of bind mounts\nPros:\n\nGreat developer experience (no need to rebuild images for every code change).\nYou can use your host editors and tools naturally.\n\nCons:\n\nBehavior differs by OS; Docker Desktop uses a VM and can have performance quirks.\nPermissions can be tricky (UID/GID mismatches).\nIn production, you often donâ€™t want arbitrary host directories exposed to containers.\n\nRule of thumb:\n\nBind mounts: local dev, debugging, special host integration.\nNamed volumes: productionâ€‘oriented persistent data and portable stacks.\n\n\n5. Tmpfs Mounts: Inâ€‘Memory Storage for Sensitive or Ephemeral Data\nTmpfs mounts keep data in RAM, never on disk.\nExample:\ndocker run -d --name cache \\\n  --tmpfs /var/cache/app \\\n  myorg/cache-heavy-app:1.0.0\n\nUse cases:\n\nSensitive temporary data that you do not want persisted.\nHighâ€‘churn temporary caches where disk I/O would be a bottleneck.\n\nBut remember:\n\nData disappears when container stops.\nIt consumes RAM, so monitor memory usage.\n\n\n6. Managing Volumes Over Time\nVolumes can accumulate just like images. You want a basic hygiene routine.\n6.1 Listing and inspecting\ndocker volume ls\ndocker volume inspect some-volume\n\nFind:\n\nWhich volumes exist.\nWhere they live on disk.\nWhich containers are using them (via inspect/Docker metadata or by crossâ€‘checking containers).\n\n6.2 Pruning unused volumes\ndocker volume prune\n# Add -f if you want to skip confirmation\n\nThis removes volumes that areÂ notÂ used by any container.\nBe careful:\n\nIf you have stopped containers that you think you might restart, removing volumes can destroy their data.\nIn dev environments, regular prune is fine if you treat data as disposable.\n\n\n7. Backups and Migration: Treat Volumes as Data Stores\nSince a volume is just a filesystem path on the host, you can back it up using standard tools. A common pattern is to run a temporary helper container that mounts the volume.\nFor example, to tar a Postgres data volume:\ndocker run --rm \\\n  -v pgdata:/data \\\n  -v &quot;$PWD:/backup&quot; \\\n  alpine:3.19 \\\n  sh -c &quot;cd /data &amp;&amp; tar czf /backup/pgdata-backup.tgz .&quot;\n\n\npgdataÂ is mounted atÂ /dataÂ inside the helper container.\nHost current directory is mounted atÂ /backup.\nYou create an archive in the host current directory.\n\nTo restore, youâ€™d reverse the process (carefully, ideally when DB is stopped).\nThis approach works for any volumeâ€‘backed data:\n\nMessage broker data.\nFile uploads.\nAnything that lives in a volume.\n\n\n8. Permission &amp; Ownership Gotchas\nContainers run processes as some user (oftenÂ rootÂ by default, but ideally a nonâ€‘root user you create). When you mount volumes or bind mounts,Â file ownership matters.\nTypical problems:\n\nHost path is owned byÂ user:groupÂ that doesnâ€™t match container user.\nContainer user cannot read/write the mounted directory.\n\nPatterns to avoid pain:\n\n\nAlign UIDs/GIDs\n\nRun container with a specific UID that matches host directory owner.\nOr create the user in the Dockerfile with the right UID.\n\n\n\nInitialize volume from container\n\nLet the container create and own its data directories on first run.\nAvoid preâ€‘creating them on host with mismatched ownership.\n\n\n\nExample in Dockerfile:\nRUN addgroup -S app &amp;&amp; adduser -S app -G app\nUSER app\n\nThen ensure that the volume mount point inside the container is writable byÂ app.\nIf you see permission denied errors on mounted paths, think: â€œWhich user am I inside the container, and who owns this directory?â€\n\n9. Volumes in Orchestrated Environments (Preview)\nIn basic Docker, volumes are relatively simple:\n\nNamed volume â†’ some directory on the host.\nBind mount â†’ exactly the host path you specify.\n\nIn Kubernetes and other orchestrators, these concepts grow into:\n\nPersistentVolumes (PV)Â andÂ PersistentVolumeClaims (PVC).\nDynamic provisioning from storage classes (EBS, GCE PD, NFS, Ceph, etc.).\nVolume plugins for cloud, network storage, etc.\n\nYour Docker mental model still holds:\n\nPods mount volumes for persistence.\nContainers inside Pods see those volumes as directories, just like your Docker containers.\nThe orchestrator manages lifecycle, scheduling, and attachment across nodes.\n\nSo the core ideaâ€”data lives outside disposable containersâ€”remains exactly the same.\n\n10. Practical Rules of Thumb for Volumes\nTo wrap this into actionable habits:\n\nNeverÂ rely on the containerâ€™s writable layer for important data; it dies with the container.\nUseÂ named volumesÂ for databases and anything you want to survive container recreation.\nUseÂ bind mountsÂ mainly for development or when you explicitly want hostâ€‘side control.\nRegularly inspect and prune unused volumes in dev environments.\nAlways think aboutÂ ownership and permissionsÂ when mounting directories.\nPractice backup/restore with at least one real service (e.g., Postgres) so itâ€™s muscle memory.\n"},"1-Devops/Docker-Mastery/5-dockerfile-mastery":{"slug":"1-Devops/Docker-Mastery/5-dockerfile-mastery","filePath":"1-Devops/Docker-Mastery/5-dockerfile-mastery.md","title":"5. Dockerfile Mastery","links":[],"tags":[],"content":"A Dockerfile is not a shell script; itâ€™s aÂ deterministic build recipeÂ that produces an immutable image. Once you understand how instructions translate into layers and how the cache behaves, you stop copyâ€‘pasting random snippets and start designing Dockerfiles like you design code: intentionally, with tradeoffs in mind.\nBelow we walk through the mental model and every core instruction, then finish by refactoring a bad Dockerfile into a productionâ€‘quality one.\n\n1. Mental model: Dockerfile as a deterministic build recipe\nTopâ€‘down execution and build graph\nWhen you runÂ docker build:\n\nDocker sends theÂ build contextÂ (files from your directory, minusÂ .dockerignore) to the daemon.\nIt parses the DockerfileÂ top to bottom.\nEach instruction (FROM, RUN, COPY, etc.) produces aÂ new layerÂ (except a few pureâ€‘metadata ones like some LABELs).\nThe final image is a stack of these layers.\n\nYou can think of it as:\n\nFROMÂ â†’ base layers.\nEveryÂ RUN/COPY/ADDÂ â†’ new layer on top.\nCMD/ENTRYPOINT/ENV/EXPOSE/USER/WORKDIR/HEALTHCHECKÂ â†’ configuration for how containers will run.\n\nBuild cache and layer invalidation\nDockerâ€™s build cache works instruction by instruction:\n\nFor each instruction, the daemon checks: â€œHave I already built this instruction before with the same inputs?â€\nInputs include:\n\nThe instruction text itself.\nThe files it touches from the build context (e.g., paths in COPY).\n\n\nIf identical, DockerÂ reuses the old layerÂ instead of reâ€‘executing.\n\nIf an instruction changes, or the files it touches change, that instruction andÂ all instructions after itÂ must be rebuilt.\nThat means:\n\nIf you doÂ COPY . .Â early, then any code change invalidates the cache for the rest of the file: expensive.\nIf you split dependency installation and source copy, you can reuse the dependency layer even when code changes.\n\nExample: better ordering for a Maven build:\n# 1. Copy pom.xml only â†’ stable if deps donâ€™t change\nCOPY pom.xml .\nRUN mvn -q -B dependency:go-offline\n\n# 2. Copy source â†’ changes more often\nCOPY src ./src\nRUN mvn -q -B package -DskipTests\n\n\nThis is the core â€œart of small layersâ€: structure your Dockerfile so thatÂ rarely changing steps go first, frequently changing steps go later.\n\n2. Core instructions, with intent\nLetâ€™s go instruction by instruction: what it really means and when to use it.\n2.1 FROM\nSets theÂ base imageÂ (starting point) for your build.\nExamples:\nFROM eclipse-temurin:21-jre-alpine    # Java runtime\nFROM node:22-alpine                   # Node.js\nFROM python:3.13-slim                 # Python\n\nWhen to use:\n\nAlways at the top of each stage.\nFor multiâ€‘stage builds, each new stage starts withÂ FROM.\n\nDesign considerations:\n\nChoose minimal but appropriate base (slim/alpine/distroless vs full OS).\nPin versions (:21-jre-alpine, notÂ :latest) for reproducibility.\nPrefer official or trusted vendor images.\n\n\n2.2 RUN\nExecutes a commandÂ at build timeÂ and commits the result as a new layer.\nExamples:\nRUN apk add --no-cache curl\n\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends curl &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\nWhen to use:\n\nInstall packages and dependencies.\nBuild/compile your application.\nPerform oneâ€‘time setup that affects filesystem contents.\n\nDesign tips:\n\nCombine related steps into a singleÂ RUNÂ when they are tightly coupled, especially for package installation + cleanup.\nAvoid leaving caches behind in earlier layers (do update/install/cleanup in theÂ sameÂ RUN).\nDonâ€™t install heavy dev tools in theÂ finalÂ runtime stage; keep them in build stages.\n\n\n2.3 COPY and ADD\nCopy files/directories into the image.\n\nCOPY:\n\nCOPY app.jar /app/app.jar\nCOPY src/ /app/src/\n\n\nADDÂ (with extra capabilities):\n\nADD data.tar.gz /data/        # autoâ€‘extracts\nADD example.com/file /tmp/file   # fetches from URL\n\nWhen to use:\n\nCOPYÂ for almost everything: itâ€™s explicit and predictable.\nADDÂ only when youÂ specifically needÂ its special behaviors (tar autoâ€‘extract or URL fetch).\n\nWe go deeper on COPY vs ADD in section 3.\n\n2.4 WORKDIR\nSets the working directory for subsequent instructions.\nWORKDIR /app\nCOPY app.jar .\nRUN ls -l\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n\nWhen to use:\n\nTo avoid full absolute paths everywhere.\nTo make your Dockerfile cleaner and commands shorter.\n\nNotes:\n\nIf the directory doesnâ€™t exist, Docker creates it.\nYou can set WORKDIR multiple times; each new one becomes the current working directory for the following instructions.\n\n\n2.5 ENV\nSets environment variables that will be availableÂ at build time and runtime.\nENV SPRING_PROFILES_ACTIVE=prod\nENV JAVA_OPTS=&quot;-Xms256m -Xmx512m&quot;\n\nWhen to use:\n\nTo bakeÂ defaultÂ runtime configuration into the image.\nTo set environment variables needed during build (thoughÂ ARGÂ is often more appropriate for buildâ€‘only values).\n\nContainers can override ENV at runtime:\nbash\ndocker run -e SPRING_PROFILES_ACTIVE=stage myorg/app:1.0.0\n\n2.6 ARG\nDefines buildâ€‘time variables available onlyÂ during build.\nARG BUILD_VERSION=dev\nARG GIT_SHA\n\nRUN echo &quot;Building version $BUILD_VERSION from $GIT_SHA&quot;\nLABEL version=&quot;$BUILD_VERSION&quot; git_sha=&quot;$GIT_SHA&quot;\n\nWhen to use:\n\nTo pass build metadata (versions, git SHA, build date).\nTo toggle build behavior (e.g.,Â ARG ENABLE_DEBUG_TOOLS=trueÂ used only in build stage).\n\nThey doÂ notÂ exist in the resulting container environment unless you explicitly copy them into ENV or labels.\nBuild with:\ndocker build \\\n  --build-arg BUILD_VERSION=1.2.3 \\\n  --build-arg GIT_SHA=abc1234 \\\n  -t myorg/app:1.2.3 .\n\nWe explore ENV vs ARG patterns in section 4.\n\n2.7 EXPOSE\nDocuments which ports the container listens on.\nEXPOSE 8080\nEXPOSE 8080 8443\n\nWhen to use:\n\nAs a form of documentation and metadata for tools.\nSoÂ docker run -PÂ knows which ports to autoâ€‘publish.\n\nImportant:\n\nEXPOSE doesÂ notÂ actually open ports on the host; you still needÂ -pÂ inÂ docker run.\n\n\n2.8 USER\nSets which user the following instructions (and the default container process) will run as.\nRUN addgroup -S app &amp;&amp; adduser -S app -G app\nUSER app\n\nWhen to use:\n\nTo drop root privileges and run your app as a nonâ€‘root user.\nAs part of basic container hardening.\n\nOnce you set USER, subsequent RUN instructions also run under that user unless you switch back.\nWe discuss security patterns more in section 6.\n\n2.9 CMD\nDefines theÂ default commandÂ (and/or arguments) to run when a container starts.\nExec form (recommended):\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\nShell form:\nCMD java -jar app.jar\nWhen to use:\n\nTo set the default command or default arguments for your container.\nExpecting that users (or orchestrators) may override it atÂ docker runÂ time.\n\nCMD is often combined with ENTRYPOINT (see section 5).\n\n2.10 ENTRYPOINT\nDefines theÂ fixed executableÂ that will always be run for this container.\nENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\nWhen to use:\n\nWhen your container is fundamentally â€œthis binaryâ€.\nTo make sure the main process is always your app, and CLI args are treated as arguments to it.\n\nTogether with CMD, it becomes:\nENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\nCMD [&quot;--spring.profiles.active=prod&quot;]\n\nAt runtime, Docker effectively runs:\njava -jar app.jar --spring.profiles.active=prod\nWe detail CMD vs ENTRYPOINT in section 5.\n\n2.11 HEALTHCHECK\nDefines a command Docker runs periodically to verify container health.\nHEALTHCHECK --interval=30s --timeout=3s --retries=3 \\\n  CMD curl -f http://localhost:8080/actuator/health || exit 1\n\nWhen to use:\n\nWhen you want a signal beyond â€œprocess is running.â€\nTo help orchestrators avoid routing traffic to unhealthy containers.\n\nGood healthchecks areÂ cheapÂ andÂ local; they shouldnâ€™t depend heavily on external systems unless thatâ€™s part of your SLA.\n\n3. COPY vs ADD â€“ the subtle traps\nYou almost always wantÂ COPY. The reason is simple:Â ADDÂ does more than just copy, and that extra behavior can surprise you.\nWhat ADD does in addition\n\nIf the source is a tar archive and the destination is a directory, ADDÂ autoâ€‘extractsÂ it.\nIf the source is a URL, ADD fetches it over HTTP/HTTPS.\n\nExample:\nADD data.tar.gz /data/   # /data contains extracted contents\nThis may look convenient, but:\n\nIt hides logic: you canâ€™t tell from the Dockerfile alone that extraction is happening.\nYou have less control over where and how it extracts.\nUsing URLs in ADD can complicate caching and reproducibility (network changes vs build cache).\n\nWhy COPY is preferred\n\nCOPY is dumb and predictable: â€œcopy exactly these files to exactly there.â€\nIt interacts clearly withÂ .dockerignore.\nYou can handle downloads and extractions explicitly in a RUN, where you also control cleanup.\n\nExample with RUN instead of ADD URL:\nRUN curl -L example.com/file.tar.gz -o /tmp/file.tar.gz &amp;&amp; \\\n    tar xzf /tmp/file.tar.gz -C /data &amp;&amp; \\\n    rm /tmp/file.tar.gz\n\nYou control:\n\nWhere the file goes.\nHow itâ€™s extracted.\nThat temporary artifact is cleaned up in the same layer.\n\nRule of thumb:Â default to COPY; reach for ADD only when you truly want its special behaviors and understand the tradeoffs.\n\n4. ENV and ARG patterns\nBuildâ€‘time vs runtime configuration\n\nARG: buildâ€‘time only, not available in running containers unless explicitly propagated.\nENV: runtime environment variables baked into the image (and also available during build after theyâ€™re set).\n\nPattern: pass build metadata via ARG, then store it via LABEL or ENV if needed at runtime.\nExample:\nARG BUILD_VERSION=dev\nARG GIT_SHA=unknown\n\nLABEL version=&quot;$BUILD_VERSION&quot; git_sha=&quot;$GIT_SHA&quot;\n\nENV APP_VERSION=$BUILD_VERSION\n\nBuild:\ndocker build \\\n  --build-arg BUILD_VERSION=1.2.3 \\\n  --build-arg GIT_SHA=abc1234 \\\n  -t myorg/app:1.2.3 .\n\nNow:\n\nThe image has labels with version and SHA.\nThe container hasÂ APP_VERSIONÂ at runtime.\n\nRuntime profiles with ENV + override\nFor a Spring Boot app:\ntext\nENV SPRING_PROFILES_ACTIVE=prod\nYou can override per environment:\n# Staging\ndocker run -e SPRING_PROFILES_ACTIVE=stage myorg/app:1.2.3\n\n# Dev\ndocker run -e SPRING_PROFILES_ACTIVE=dev myorg/app:1.2.3\n\nDesign principle:\n\nDonâ€™t rebuild images just to change environmentâ€‘specific settings.\nUse ENV for defaults; override via runtime env variables in Compose/Kubernetes.\n\n\n5. CMD vs ENTRYPOINT patterns\nFixed executable vs overridable parameters\nThink of:\n\nENTRYPOINTÂ = binary to always run.\nCMDÂ = default arguments, overridable at runtime.\n\nCanonical pattern:\nENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\nCMD [&quot;--spring.profiles.active=prod&quot;]\n\nThen:\n# uses default profile=prod\ndocker run myorg/app:1.0.0\n\n# override profile with CLI arg\ndocker run myorg/app:1.0.0 --spring.profiles.active=stage\n\n\nDocker effectively does:\njava -jar app.jar --spring.profiles.active=stage\nCommon mistakes\n\n\nPutting everything in CMD and then overriding the whole thing at runtime:\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;,&quot;--spring.profiles.active=prod&quot;]\nIf someone runsÂ docker run myorg/app:1.0.0 my-custom-command, your app doesnâ€™t run at all.\n\n\nUsing shell form and breaking signal handling:\nCMD java -jar app.jar\nPID 1 isÂ /bin/sh -c, not your app; SIGTERM/SIGINT may not propagate cleanly.\n\n\nBetter:Â use exec form for both ENTRYPOINT and CMD.\n\n6. Securityâ€‘aware Dockerfile writing\nThe Dockerfile is your first line of defense. A few simple patterns dramatically reduce risk.\n6.1 Nonâ€‘root user\nCreate a dedicated user and drop root:\nFROM eclipse-temurin:21-jre-alpine\n\nRUN addgroup -S app &amp;&amp; adduser -S app -G app\n\nWORKDIR /app\nCOPY app.jar /app/app.jar\n\nUSER app\n\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n\nBenefits:\n\nEven if your app is compromised, the attacker has fewer privileges in the container.\nLimits damage from misconfigurations.\n\nRemember to ensure directories your app writes to are writable byÂ app.\n6.2 No secrets in images\nNever:\n\nCOPY .env /app/.env\nENV DB_PASSWORD=supersecretÂ in a public or widely shared image.\n\nBetter:\n\nInject secrets at runtime via environment variables, secret managers, or orchestrator mechanisms (Kubernetes Secrets, etc.).\nRestrict secrets to runtime configuration, not immutable image configuration.\n\n6.3 Minimal surface area\n\nUse minimal base images (alpine, slim, distroless).\nAvoid unnecessary tools (curl, ping, etc.) in theÂ finalÂ runtime image; keep them only in build or debug images.\n\nThe smaller and simpler your image, the fewer attack vectors and CVEs youâ€™ll carry.\n\n7. Example: refactoring a bad Dockerfile\n7.1 NaÃ¯ve Dockerfile\nFROM openjdk:21\n\nWORKDIR /app\nCOPY . .\nRUN mvn package\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;target/app.jar&quot;]\n\nIssues:\n\nHeavy base image (full OS + JDK).\nCopies entire context (includingÂ .git,Â target, docs, etc.).\nPoor cache usage: any code change invalidates Maven dependency downloads.\nRuns as root.\nNo explicit healthcheck or environment defaults.\nBuild tools (Maven, JDK) are in the same image used in production.\n\n7.2 Refactored, productionâ€‘grade Dockerfile\n# Stage 1: build\nFROM eclipse-temurin:21-jdk-alpine AS build\nWORKDIR /app\n\n# Install dependencies based on pom.xml (good cache usage)\nCOPY pom.xml .\nRUN mvn -q -B dependency:go-offline\n\n# Copy source and build\nCOPY src ./src\nRUN mvn -q -B package -DskipTests\n\n# Stage 2: runtime\nFROM eclipse-temurin:21-jre-alpine\n\n# Create non-root user\nRUN addgroup -S app &amp;&amp; adduser -S app -G app\n\nWORKDIR /app\nCOPY --from=build /app/target/app.jar app.jar\n\n# Runtime defaults\nENV SPRING_PROFILES_ACTIVE=prod\n\nUSER app\nEXPOSE 8080\n\n# Healthcheck (simple example)\nHEALTHCHECK --interval=30s --timeout=3s --retries=3 \\\n  CMD curl -f http://localhost:8080/actuator/health || exit 1\n\n# Binary + default args pattern\nENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\nCMD [&quot;--spring.profiles.active=prod&quot;]\n\nWhat improved:\n\nMultiâ€‘stage build: JDK + Maven only in build stage; final image has JRE only.\nSmaller, more secure runtime image.\nBetterÂ cache behavior: dependencies cached separately from source.\nNonâ€‘root user.\nEXPOSE and HEALTHCHECK provide standard metadata and health signal.\nENTRYPOINT/CMD split allows overriding profile without rewriting command.\n\nThis is â€œDockerfile masteryâ€: the same basic goal (run a Spring Boot JAR) implemented in a way that respects performance, security, maintainability, and observability."},"1-Devops/Docker-Mastery/6-multi-stage-docker-builds":{"slug":"1-Devops/Docker-Mastery/6-multi-stage-docker-builds","filePath":"1-Devops/Docker-Mastery/6-multi-stage-docker-builds.md","title":"6. Multiâ€‘Stage Docker Builds","links":[],"tags":[],"content":"Multiâ€‘stage builds exist to solve a very concrete problem: images that are huge, slow to ship, and full of compilers and dev tools that have no business being in production. You keep all the heavy build tooling in one stage, and ship only the small, clean runtime stage to prod.\n\n1. The problem: fat images with build tools inside\nIn a naÃ¯ve Dockerfile, you often see something like:\nFROM node:22\n\nWORKDIR /app\nCOPY . .\nRUN npm install\nRUN npm run build\nCMD [&quot;npm&quot;,&quot;start&quot;]\n\nor for Java:\nFROM maven:3.9-eclipse-temurin-21\n\nWORKDIR /app\nCOPY . .\nRUN mvn -q -B package\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;target/app.jar&quot;]\n\n\nWhatâ€™s wrong here?\n\nJDK, Maven, Node toolchainÂ are all present in the final image.\nYouâ€™re shipping:\n\nPackage managers (npm, apt, Maven).\nCompilers and build tools.\nDev dependencies (e.g., node_modules with test tooling).\n\n\nImpact:\n\nPull time: every node in your cluster pulls a big image.\nDisk usage: each node stores these large layers.\nSecurity: more binaries and libraries â†’ more potential CVEs â†’ more noise in vulnerability scans.\n\n\n\nThe app only needs theÂ built artifactÂ (JAR, binary, static assets), not the entire toolbox used to produce it.\n\n2. Concept: separate build stage from runtime stage\nMultiâ€‘stage builds let you defineÂ multiple FROMsÂ in a single Dockerfile. Each FROM starts a new â€œstageâ€ with its own filesystem. You can then selectively copy build outputs from earlier stages into a minimal final stage.\nCore mechanics:\nFROM some-builder-image AS build\n# ... build commands ...\n\nFROM small-runtime-image\n# Copy artifacts from build stage\nCOPY --from=build /path/in/build-stage /path/in/runtime\n# ... runtime config ...\n\nKey elements:\n\nAS buildÂ names the stageÂ build. You can choose any name (e.g.,Â builder,Â compile).\nCOPY --from=buildÂ pulls filesÂ onlyÂ from that stageâ€™s filesystem into the current stage.\nEarlier stages donâ€™t exist in the final image unless you copy data explicitly.\n\nWhy itâ€™s â€œcheapâ€:\n\nDocker still layers everything efficiently.\nStages share cached layers like normal builds.\nOnly theÂ last stageÂ becomes the final image thatâ€™s pushed/pulled in normal workflows.\n\nSo you get:\n\nFull power of a heavy build environment.\nA clean, minimal runtime image used in production.\n\n\n3. Languageâ€‘specific examples\n3.1 Node: build static assets, serve with Nginx\nGoal: use Node only to build the frontend, then serve static files with Nginx.\n# Stage 1: build\nFROM node:22-alpine AS build\nWORKDIR /app\n\n# Install dependencies\nCOPY package*.json ./\nRUN npm ci\n\n# Copy source and build\nCOPY . .\nRUN npm run build\n\n# Stage 2: runtime\nFROM nginx:1.25-alpine\n\n# Copy built assets from build stage\nCOPY --from=build /app/dist /usr/share/nginx/html\n\nEXPOSE 80\nCMD [&quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot;]\n\nWhat you achieve:\n\nNode, npm, devDependencies stay in theÂ buildÂ stage.\nFinal image is just Nginx + static files.\nImage is smaller, faster to start, simpler to scan.\n\n\n3.2 Java: JDK + Maven â†’ JREâ€‘only final\nYou saw a version of this earlier; hereâ€™s a focused multiâ€‘stage example.\n# Stage 1: build\nFROM eclipse-temurin:21-jdk-alpine AS build\nWORKDIR /app\n\n# Dependencies\nCOPY pom.xml .\nRUN mvn -q -B dependency:go-offline\n\n# Source and build\nCOPY src ./src\nRUN mvn -q -B package -DskipTests\n\n# Stage 2: runtime\nFROM eclipse-temurin:21-jre-alpine\n\nWORKDIR /app\nCOPY --from=build /app/target/app.jar app.jar\n\nEXPOSE 8080\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n\n\nProperties:\n\nJDK + Maven only inÂ buildÂ stage.\nFinal runtime image has just a JRE and your JAR.\nMuch smaller than usingÂ maven:...Â directly as the only base.\n\nYou can then further harden the runtime stage (nonâ€‘root user, healthcheck, ENV, etc.).\n\n3.3 Go: build inÂ golang, run inÂ scratchÂ orÂ distroless\nGo produces static binaries easily, which are perfect for tiny runtime images.\n# Stage 1: build\nFROM golang:1.23-alpine AS build\nWORKDIR /app\n\nCOPY go.mod go.sum ./\nRUN go mod download\n\nCOPY . .\nRUN CGO_ENABLED=0 GOOS=linux GOARCH=amd64 go build -o app\n\n# Stage 2: runtime (scratch)\nFROM scratch\n\n# Copy binary and (optionally) CA certs if needed\nCOPY --from=build /app/app /app/app\n\n# Typically set a working dir and maybe certs or config\nWORKDIR /app\n\nENTRYPOINT [&quot;/app/app&quot;]\n\n\nAdvantages:\n\nFinal image includes only your compiled binary (and maybe CA certs).\nNo shell, no package manager, no extra libs.\nExtremely small, fast, and minimal attack surface.\n\nAlternatively, you can use a distroless base if you need some minimal runtime libs:\nFROM gcr.io/distroless/base\nCOPY --from=build /app/app /app/app\nWORKDIR /app\nENTRYPOINT [&quot;/app/app&quot;]\n\n4. Optimizing build cache in multiâ€‘stage builds\nMultiâ€‘stage doesnâ€™t magically fix bad cache usage; you still need to order instructions smartly.\n4.1 General pattern: dependencies first, then source\nIn build stages for dependencyâ€‘heavy languages (Java, Node), do:\n\nCopyÂ dependency manifestÂ (pom.xml, package.json).\nInstall dependencies.\nCopy the rest of the source.\nBuild.\n\nJava example:\nFROM eclipse-temurin:21-jdk-alpine AS build\nWORKDIR /app\n\n# 1. Dependencies layer\nCOPY pom.xml .\nRUN mvn -q -B dependency:go-offline\n\n# 2. Source layer\nCOPY src ./src\nRUN mvn -q -B package -DskipTests\n\nNode example:\nFROM node:22-alpine AS build\nWORKDIR /app\n\nCOPY package*.json ./\nRUN npm ci\n\nCOPY . .\nRUN npm run build\n\nWhy this helps:\n\nAs long as your dependency file doesnâ€™t change, the dependency install layer is reused.\nSmall code changes only invalidate the later layers, keeping builds much faster.\n\n4.2 Buildâ€‘only vs runtime files\nMultiâ€‘stage gives you a natural separation:\n\nBuild stage: everything needed to compile/build (full source, tests, docs, etc.).\nRuntime stage: only whatâ€™s needed to run (binaries, JARs, static assets, configs).\n\nYou consciously choose what to copy withÂ COPY --from, so:\n\nYou donâ€™t accidentally ship test files or docs.\nYou donâ€™t ship package lockfiles or build caches.\n\n\n5. Security and compliance benefits\nMultiâ€‘stage builds are not just about size and speed; they directly improve security posture.\n5.1 Smaller attack surface\n\nNo compilers or interpreters in the runtime image.\nNo build tools like Maven, npm, pip, etc.\nFewer system libraries.\n\nIf an attacker breaks into the container, thereâ€™sÂ less toolingÂ to leverage for lateral movement or exploitation.\n5.2 Cleaner vulnerability scans\nSecurity tools scan all packages and libraries inside an image. If you ship a giant build image:\n\nYou see CVEs for compilers, build tools, and things that never run in production.\nNoise makes it harder to focus on real runtime risks.\n\nWith a minimal runtime image:\n\nScan results mostly reflect the libraries your app actually uses in production.\nRemediation and patching focus becomes much clearer.\n\n5.3 Easier upgrades\nWhen you separate build and runtime:\n\nYou can update the build image (new compiler, new Maven)Â withoutÂ changing the runtime image base.\nOr update runtime base (for CVE patches) without touching build toolchain right away.\n\nThis decoupling can be helpful in regulated environments where change control for build and runtime flows differ.\n\n6. Migration story: from singleâ€‘stage to multiâ€‘stage\nLetâ€™s take a concrete, legacy singleâ€‘stage Dockerfile and evolve it step by step.\n6.1 Legacy singleâ€‘stage Java Dockerfile\nFROM maven:3.9-eclipse-temurin-21\n\nWORKDIR /app\nCOPY . .\nRUN mvn -q -B package -DskipTests\nEXPOSE 8080\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;target/app.jar&quot;]\n\nProblems:\n\nHeavy image: includes Maven and full JDK in production.\nCopies entire context (including target, .git, etc.).\nPoor cache usage (COPY . . early).\nNo separation of build vs runtime.\n\n6.2 Step 1: introduce build and runtime stages\n# Stage 1: build\nFROM maven:3.9-eclipse-temurin-21 AS build\nWORKDIR /app\nCOPY . .\nRUN mvn -q -B package -DskipTests\n\n# Stage 2: runtime\nFROM eclipse-temurin:21-jre-alpine\nWORKDIR /app\nCOPY --from=build /app/target/app.jar app.jar\nEXPOSE 8080\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n\nNow:\n\nFinal image is based on JRE only.\nMaven &amp; JDK live only in the build stage.\n\n6.3 Step 2: improve cache usage in the build stage\nRefine the build stage:\nFROM maven:3.9-eclipse-temurin-21 AS build\nWORKDIR /app\n\n# Dependencies first\nCOPY pom.xml .\nRUN mvn -q -B dependency:go-offline\n\n# Then source\nCOPY src ./src\nRUN mvn -q -B package -DskipTests\n\nThis avoids reâ€‘downloading dependencies on every code change.\n6.4 Step 3: slim down the builder base (optional)\nIf you want a lighter builder:\nFROM eclipse-temurin:21-jdk-alpine AS build\n# install Maven manually OR use a Maven wrapper\nWORKDIR /app\n\nCOPY mvnw pom.xml ./\nCOPY .mvn .mvn\nRUN ./mvnw -q -B dependency:go-offline\n\nCOPY src ./src\nRUN ./mvnw -q -B package -DskipTests\n\nBuild still happens inÂ buildÂ stage, but with a smaller JDK + Maven wrapper instead of theÂ maven:Â base.\n6.5 Step 4: harden the runtime stage\nNow secure the runtime stage as well:\nFROM eclipse-temurin:21-jre-alpine\n\n# Create non-root user\nRUN addgroup -S app &amp;&amp; adduser -S app -G app\n\nWORKDIR /app\nCOPY --from=build /app/target/app.jar app.jar\n\nENV SPRING_PROFILES_ACTIVE=prod\n\nUSER app\nEXPOSE 8080\n\nHEALTHCHECK --interval=30s --timeout=3s --retries=3 \\\n  CMD curl -f http://localhost:8080/actuator/health || exit 1\n\nENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\nCMD [&quot;--spring.profiles.active=prod&quot;]\n\n\nFinal result:\n\nMultiâ€‘stage build.\nOptimized caching.\nMinimal runtime image.\nNonâ€‘root user.\nHealthcheck and ENV defaults.\n\nTheÂ behaviorÂ (run a Spring Boot JAR) is the same, but the image is faster, smaller, and safer."},"1-Devops/Docker-Mastery/7-docker-security-essentials":{"slug":"1-Devops/Docker-Mastery/7-docker-security-essentials","filePath":"1-Devops/Docker-Mastery/7-docker-security-essentials.md","title":"7. Docker Security Essentials","links":[],"tags":[],"content":"Containers make deployment easier, but they donâ€™t magically make things safe. A container is just a process on a shared kernel with some isolation. Security comes from how youÂ buildÂ images and how youÂ runÂ containers, not from Dockerâ€™s existence alone.\n\n1. Threat model for containers\nShared kernel: what it implies\n\nContainers areÂ notÂ VMs: there is no separate kernel per container.\nAll containers and the host share theÂ same Linux kernel, using:\n\nNamespaces for isolation (PID, network, mount, etc.).\ncgroups for resource limits.\n\n\n\nImplications:\n\nA kernel bug or misconfiguration can potentially be exploited from inside a container.\nIf a container breaks out of its namespaces, it may affect other containers or the host.\n\nâ€œItâ€™s in a containerâ€ is not a magic boundary\nCommon misconceptions:\n\nâ€œIf itâ€™s in a container, itâ€™s safe even if running as root.â€\nâ€œWe can just mount anything from the host; Docker will protect us.â€\n\nReality:\n\nRoot inside a container can often do dangerous things if the container is misconfigured (e.g., privileged mode, host mounts).\nVolumes and bind mounts can expose sensitive host paths directly.\nDocker is aÂ convenience, not a security sandbox; treat containers as apps running on the host, with extra isolation but not perfect.\n\nMindset:\nAssume an attacker can getÂ code execution inside the container. Your job is to limit what they can doÂ from there.\n\n2. User and permissions\nNonâ€‘root users inside containers\nBy default, many base images run asÂ root. If your app is compromised, the attacker gets root inside the container.\nBetter pattern:\n\nCreate a dedicated user in the Dockerfile.\nSwitch to it withÂ USER.\n\nExample:\nFROM eclipse-temurin:21-jre-alpine\n\n# Create group and user\nRUN addgroup -S app &amp;&amp; adduser -S app -G app\n\nWORKDIR /app\nCOPY app.jar /app/app.jar\n\nUSER app\n\nCMD [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\n\n\nBenefits:\n\nThe app runs with limited privileges.\nEven if an exploit lands, it canâ€™t directly perform many rootâ€‘level operations.\n\nFile ownership with volumes and bind mounts\nVolumes and bind mounts introduce another dimension:Â who owns the files.\nTypical issues:\n\nHost directory owned by a user with UID/GID that doesnâ€™t match the containerâ€™s user.\nContainer process gets â€œpermission deniedâ€ when trying to write.\n\nPatterns:\n\nAlign UIDs: run container with a user whose UID/GID matches host directory owner, or adjust host directory permissions.\nLet the container initialize its own volume:\n\nFirst run as root to set ownership, then drop to nonâ€‘root, or\nUse an init container step (in orchestration) to prepare permissions.\n\n\n\nExample volume pattern:\ndocker run -d --name db \\\n  -e POSTGRES_PASSWORD=secret \\\n  -v pgdata:/var/lib/postgresql/data \\\n  postgres:15-alpine\n\nHere, Postgres image is usually prepared to manage its own data directory permissions. For your custom images, make sure theÂ USERÂ has write permission to any mounted paths.\n\n3. Capabilities and seccomp (conceptual)\nLinux capabilities: fineâ€‘grained root\nIn Linux, â€œrootâ€ privileges are split intoÂ capabilitiesÂ (e.g.,Â NET_ADMIN,Â SYS_ADMIN). Docker containers get aÂ reduced setÂ of capabilities by default, but often more than they strictly need.\nInstead of allâ€‘orâ€‘nothing root, you can drop capabilities and only add back whatâ€™s required.\nExample pattern:\ndocker run -d --name web \\\n  --cap-drop=ALL \\\n  --cap-add=NET_BIND_SERVICE \\\n  -p 80:80 \\\n  nginx:1.25-alpine\n\n\n\n--cap-drop=ALLÂ removes all capabilities.\n--cap-add=NET_BIND_SERVICEÂ permits binding to low ports (&lt;1024) without full root.\n\nThis is a powerful â€œleast privilegeâ€ control.\nSeccomp (high level)\nSeccomp (secure computing mode) lets you filter whichÂ syscallsÂ a process can make. Docker ships with a default seccomp profile that blocks some risky syscalls.\nYou rarely write seccomp profiles by hand at first, but you shouldÂ knowÂ that:\n\nDocker can block dangerous syscalls by default.\nYou can opt into different profiles or, in rare cases, relax them if your app needs unusual syscalls.\n\nBasic principle:\nDonâ€™t disable the default seccomp profile unless you really know what youâ€™re doing.\n\n4. Image hygiene\nSecurity also comes fromÂ whatâ€™s inside your image.\nKeep base images updated\n\nBase images inherit vulnerabilities from the underlying OS and libraries.\nYou should periodically:\n\nRebuild images from updated base tags.\nTrack upstream base image changelogs and security advisories.\n\n\n\nUsing versioned tags likeÂ eclipse-temurin:21-jre-alpineÂ is good, but you still need toÂ pull updated variantsÂ as they are released and rebuild your images.\nInstall only what you need, clean up after\nEvery extra package:\n\nIncreases attack surface.\nIncreases the number of things scanners will flag.\n\nPatterns:\n\nAvoid â€œkitchen sinkâ€ installs (apt-get install -y nano vim curl wget net-tools ...) in runtime images.\nUse minimal installs and remove package caches.\n\nExample for Debian/Ubuntu based images:\nRUN apt-get update &amp;&amp; \\\n    apt-get install -y --no-install-recommends curl &amp;&amp; \\\n    rm -rf /var/lib/apt/lists/*\n\nFor Alpine:\nRUN apk add --no-cache curl\nCombine install and cleanup in oneÂ RUNÂ so caches donâ€™t remain in previous layers.\n\n5. Runtime hardening flags\nBeyond the Dockerfile,Â runâ€‘time optionsÂ give you strong guardrails.\nReadâ€‘only root filesystem\nIf your app doesnâ€™t need to write to the root filesystem, you can mount it readâ€‘only and use volumes for writable paths.\ndocker run -d --name api \\\n  --read-only \\\n  -v app-tmp:/tmp \\\n  myorg/api:1.0.0\n\n\nEffects:\n\nThe containerâ€™s root filesystem is readâ€‘only.\nApp can still write toÂ /tmpÂ (a dedicated volume).\nAttacks that try to drop binaries or modify configuration files on root FS are harder.\n\nNo new privileges\n--security-opt no-new-privilegesÂ prevents processes from gaining more privileges (e.g., via setuid binaries).\ndocker run -d --name api \\\n  --security-opt no-new-privileges \\\n  myorg/api:1.0.0\n\n\nThis enforces a strong â€œno escalationâ€ rule even if some binaries have setuid bits.\nTightening Nginx as an example\nPutting it together:\ndocker run -d --name web \\\n  -p 80:80 \\\n  --read-only \\\n  -v web-logs:/var/log/nginx \\\n  --cap-drop=ALL \\\n  --cap-add=NET_BIND_SERVICE \\\n  --security-opt no-new-privileges \\\n  nginx:1.25-alpine\n\nWeâ€™ve:\n\nDropped all capabilities except whatâ€™s needed to bind port 80.\nMade filesystem readâ€‘only except for logs.\nPrevented privilege escalation.\n\nThis is a realistic baseline for a simple web server.\n\n6. Secrets handling basics\nWhy not bake secrets into images\nBad patterns:\n\nENV DB_PASSWORD=supersecretÂ in the Dockerfile.\nCopyingÂ .envÂ or config with credentials into the image.\nChecking these files into source control.\n\nProblems:\n\nSecret is now in every environment where the image lands.\nAnyone with access to the image registry can potentially read it.\nRotating the secret requires rebuilding images.\n\nBetter approaches (highâ€‘level)\nThe principle:Â inject secrets at runtime, not build time.\nCommon patterns:\n\nRuntime environment variables:\n\ndocker run -e DB_PASSWORD=...\nBut still sensitive; use restricted environment (orchestrator secrets) rather than plain text in scripts.\n\n\nOrchestrator secret mechanisms:\n\nDocker Swarm secrets.\nKubernetes Secrets (mounted as files or env vars).\nCloud provider secret managers (AWS Secrets Manager, GCP Secret Manager, Vault, etc.).\n\n\n\nIn all cases:\n\nThe Dockerfile shouldÂ neverÂ hardcode secrets.\nImages should be reusable across environments; secrets are part ofÂ deployment configuration, not the artifact itself.\n\n\nPutting it all together: baseline security checklist\nWhen you design Docker images and containers, treat this as your default checklist:\n\nThreat model:\n\nRemember containers share the host kernel; donâ€™t assume perfect containment.\n\n\nUsers &amp; permissions:\n\nCreate a nonâ€‘root user in Dockerfile,Â USERÂ to it.\nEnsure volumes and bind mounts respect that userâ€™s permissions.\n\n\nCapabilities &amp; seccomp:\n\nDrop capabilities by default, add only what you need.\nDonâ€™t casually disable Dockerâ€™s default seccomp profile.\n\n\nImage hygiene:\n\nUse minimal, versioned base images.\nInstall only necessary packages; clean caches.\nRebuild on base image updates.\n\n\nRuntime hardening:\n\nUseÂ --read-onlyÂ and dedicated writable volumes where possible.\nApplyÂ --security-opt no-new-privileges.\nAvoid unnecessary privileged containers and host mounts.\n\n\nSecrets:\n\nNever bake secrets into the image.\nUse runtime injection and secret management systems.\n\n\n\nOnce this baseline becomes automatic, youâ€™ll naturally design Docker setups that are much closer to what SREs and security teams expect in real production environments."},"1-Devops/Docker-Mastery/8-docker-registries-and-tagging":{"slug":"1-Devops/Docker-Mastery/8-docker-registries-and-tagging","filePath":"1-Devops/Docker-Mastery/8-docker-registries-and-tagging.md","title":"8. Docker Registries and Tagging","links":[],"tags":[],"content":"Containers are just a packaging format; theÂ realÂ control over what runs in each environment comes from your registry and tagging strategy. If you treat images like real versioned artifacts (not random blobs named â€œlatestâ€), debugging, rollbacks, and audits become much simpler.\n\n1. Mental model: registry as artifact repository\nThink of a container registry like aÂ Maven repositoryÂ orÂ Git server, but for images.\n\nPublic registries\n\nDocker Hub (docker.io/library/nginx).\nPublic GHCR (GitHub Container Registry), etc.\nGreat for base images and openâ€‘source artifacts.\n\n\nPrivate registries\n\nAWS ECR (123456789012.dkr.ecr.us-east-1.amazonaws.com/myorg/api).\nGCP Artifact Registry, Azure ACR.\nSelfâ€‘hosted: Harbor, GitLab Registry, etc.\nUsed for your orgâ€™s internal services.\n\n\n\nAn image name has three parts:\n[REGISTRY/]NAMESPACE/REPOSITORY:TAG\nExamples:\n\nnginx:1.25-alpine\n\nRegistry: default Docker Hub.\nNamespace:Â libraryÂ (implicit).\nRepository:Â nginx.\nTag:Â 1.25-alpine.\n\n\nmy-registry.example.com/myorg/payment-service:1.0.0\n\nRegistry:Â my-registry.example.com.\nNamespace:Â myorg.\nRepository:Â payment-service.\nTag:Â 1.0.0.\n\n\n\nNamespace + repository (myorg/service) should reflectÂ ownership and purpose, like Maven group/artifact.\n\n2. Tagging strategies that donâ€™t ruin your life\nTags areÂ labelsÂ pointing to an image ID. Theyâ€™re not immutable by themselves; you decide how to use them.\nSemantic versions and build numbers\nTreat images like versioned releases:\n\nSemantic versions:Â 1.0.0,Â 1.0.1,Â 2.0.0.\nBuild identifiers:Â 1.0.0-20260201.1,Â 1.0.0+build.42.\nGit SHA tags:Â app:git-abc1234.\n\nCommon pattern after building an image from commitÂ abc1234:\ndocker tag myorg/api:build \\\n  myorg/api:1.2.3 \\\n  myorg/api:1.2 \\\n  myorg/api:git-abc1234\n\n\nNow you can refer to theÂ exactÂ artifact by any of these tags, butÂ git-abc1234Â is uniquely tied to a commit.\nUsingÂ latestÂ responsibly\nlatestÂ is just another tag; Docker doesnâ€™t treat it specially. Problems arise when:\n\nDifferent teams assumeÂ latestÂ means different things.\nlatestÂ points to different builds across environments.\n\nRecommended:\n\nInÂ production, avoid usingÂ latestÂ in deployments. Use explicit version tags.\nIf you keepÂ latestÂ at all, treat it as â€œmost recent successful stable build,â€ and be disciplined about how itâ€™s updated.\n\nMappingÂ devÂ /Â stagingÂ /Â prodÂ to tags\nInstead of rebuilding for each environment, useÂ tags to represent promotion level:\n\nmyorg/api:1.2.3-dev\nmyorg/api:1.2.3-staging\nmyorg/api:1.2.3-prod\n\nOr keep environment tags separate from version tags:\n\nmyorg/api:1.2.3Â is the version.\nmyorg/api:prodÂ points to whatever version is currently live in production (you move theÂ prodÂ tag during promotion).\n\nThis lets you answer:\n\nâ€œWhat version is in prod?â€ â†’ inspectÂ myorg/api:prod.\nâ€œWhatâ€™s running in staging?â€ â†’Â myorg/api:staging.\n\n\n3. Push/pull workflow in CI/CD\nA clean CI/CD flow treats image building as part of the pipeline and relies on tagging for traceability.\nExample pipeline flow\nGiven a commitÂ abc1234Â for versionÂ 1.2.3:\n\nBuild image\ndocker build -t myorg/api:build .\nTag image with metadata\n\nGIT_SHA=abc1234\nVERSION=1.2.3\n\ndocker tag myorg/api:build myorg/api:${VERSION}\ndocker tag myorg/api:build myorg/api:${VERSION}-${GIT_SHA}\ndocker tag myorg/api:build myorg/api:git-${GIT_SHA}\n\n\n\nPush to registry\n\ndocker push myorg/api:${VERSION}\ndocker push myorg/api:${VERSION}-${GIT_SHA}\ndocker push myorg/api:git-${GIT_SHA}\n\n\nDeploy by tag\n\nDev: deployÂ myorg/api:${VERSION}-${GIT_SHA}.\nStaging: when tests pass, deploy theÂ same tag.\nProd: promote that tag again.\n\n\n\nAt no point do you rebuild the image for each environment; you always deploy theÂ same artifact.\nWhy this is powerful\n\nEvery running container is traceable back to a Git SHA and build.\nLogs, metrics, and incidents can be tied to a specific image version.\nWhen a bug is found, you know exactly which version introduced it.\n\n\n4. Promoting images across environments\nA common antiâ€‘pattern: build separate images for dev, staging, prod, even when code is identical.\nBetter:Â build once, promote via tags.\nPromotion via retagging\nStart withÂ myorg/api:1.2.3-abc1234Â as your canonical build tag.\n\nDev deployment: useÂ myorg/api:1.2.3-abc1234.\nOnce validated,Â retagÂ in the registry or via CI:\n\ndocker tag myorg/api:1.2.3-abc1234 myorg/api:staging\ndocker push myorg/api:staging\n\n\n\nFor production, do:\n\ndocker tag myorg/api:1.2.3-abc1234 myorg/api:prod\ndocker push myorg/api:prod\n\nNow:\n\nprodÂ represents â€œcurrent production image.â€\nstagingÂ represents â€œcurrent staging image.â€\nBoth point back to the same underlying build and Git SHA.\n\nBenefits for debugging and rollback\nWhen something breaks in prod:\n\nYou can quickly see which tag/version is running.\nTo roll back, pointÂ prodÂ back to a previous known-good tag:\n\ndocker tag myorg/api:1.2.2-xyz9876 myorg/api:prod\ndocker push myorg/api:prod\n\nYour deployment tooling then picks up the updatedÂ prodÂ tag and redeploys, without rebuilding.\nThis is very similar to promoting artifacts through environments in Maven or artifact repositories, but with container images.\n\n5. Registry access and auth basics\nLogging into registries\nFor private registries, you authenticate before pushing or pulling:\ndocker login my-registry.example.com\n# prompts for username/password or token\n\n\nIn CI:\n\nUse access tokens or service accounts instead of personal credentials.\nConfigure credentials as pipeline secrets and inject them into the build job.\n\nImage names in orchestrators\nWhen you use Docker Compose, Swarm, or Kubernetes, you reference the same image names and tags:\n\nCompose:\n\nservices:\n  api:\n    image: my-registry.example.com/myorg/api:1.2.3-abc1234\n\n\nKubernetes Deployment:\n\nspec:\n  template:\n    spec:\n      containers:\n      - name: api\n        image: my-registry.example.com/myorg/api:1.2.3-abc1234\n\nThe orchestrator:\n\nAuthenticates against the registry (via imagePullSecrets or node IAM roles).\nPulls the specified image tag onto the nodes.\n\nSo yourÂ tagging strategyÂ directly affects:\n\nReproducibility of deployments.\nHow easy it is to roll back, audit, and debug.\n\n\nPractical checklist for registries &amp; tagging\nWhen designing your registry/tagging setup, aim for:\n\nConsistent naming:Â registry/namespace/service:tagÂ with clear org/service mapping.\nVersioned tags: semantic versions plus build/Git SHA tags.\nMinimal use ofÂ latest: avoid it entirely in production manifests.\nPromotion via tags, not rebuilds: build once, promote the same artifact through dev â†’ staging â†’ prod.\nTraceability: every running containerâ€™s image tag lets you find the exact Git commit and build.\n"},"1-Devops/Docker-Mastery/9-docker-compose":{"slug":"1-Devops/Docker-Mastery/9-docker-compose","filePath":"1-Devops/Docker-Mastery/9-docker-compose.md","title":"9. Docker Compose","links":[],"tags":[],"content":"Running one container withÂ docker runÂ is fine. Running four containers (API, DB, cache, UI) with long commands and manual networks is pain.Â Docker ComposeÂ solves this by letting you describe your whole stack in a single YAML file and manage it with a few short commands.\n\n1. Why Compose exists\nThe pain of rawÂ docker run\nA realistic microservice stack might need for each container:\n\n--name\n-pÂ port mappings\n--network\n-eÂ environment variables\n-vÂ volumes\n\nFor anÂ apiÂ +Â dbÂ stack, thatâ€™s already two ugly commands you must remember and retype, and it gets much worse with more services. Recreating the same environment on another machine is errorâ€‘prone.\nYAML as â€œdocker run on steroidsâ€\nDocker Compose lets you write aÂ declarativeÂ description of your stack inÂ docker-compose.yml:\n\nEach service describes its image, ports, environment, volumes, networks.\nCompose takes care of:\n\nCreating the network(s).\nStarting services in the right order (withÂ depends_onÂ as a hint).\nWiring DNS names (service names) for containerâ€‘toâ€‘container communication.\n\n\n\nOne file becomes yourÂ local environment definitionÂ that you can versionâ€‘control and share.\n\n2. Core concepts\nAt a high level, Compose YAML has three main sections you use most often:\n\nservicesÂ â€“ each container you want to run.\nvolumesÂ â€“ named volumes for persistent storage.\nnetworksÂ â€“ logical networks connecting services.\n\nExample skeleton:\nversion: &quot;3.9&quot;\n\nservices:\n  api:\n    image: myorg/orders-api:1.0.0\n    ports:\n      - &quot;8080:8080&quot;\n    environment:\n      DB_HOST: db\n\n  db:\n    image: postgres:15-alpine\n    environment:\n      POSTGRES_PASSWORD: secret\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n\nvolumes:\n  pgdata:\n\nnetworks:\n  # optional to declare explicitly; Compose creates a default one if omitted\n\nMental model:\n\nservices.apiÂ andÂ services.dbÂ are like namedÂ docker runÂ definitions.\nvolumes.pgdataÂ is likeÂ docker volume create pgdata.\nCompose automatically creates a dedicatedÂ networkÂ for this stack, and connects all services to it.\n\nOneÂ docker-compose.ymlÂ = oneÂ stackÂ (your â€œlocal environmentâ€).\n\n3. Walking through a simple stack\nLetâ€™s build a concreteÂ apiÂ +Â dbÂ stack.\nCompose file\nversion: &quot;3.9&quot;\n\nservices:\n  db:\n    image: postgres:15-alpine\n    container_name: orders-db\n    environment:\n      POSTGRES_DB: orders\n      POSTGRES_USER: orders_user\n      POSTGRES_PASSWORD: secret\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n    ports:\n      - &quot;5432:5432&quot;\n\n  api:\n    image: myorg/orders-api:1.0.0\n    container_name: orders-api\n    depends_on:\n      - db\n    environment:\n      DB_HOST: db        # service name, not host IP\n      DB_PORT: 5432\n      DB_NAME: orders\n      DB_USER: orders_user\n      DB_PASSWORD: secret\n    ports:\n      - &quot;8080:8080&quot;\n\nvolumes:\n  pgdata:\n\nWhat Compose does when you runÂ docker compose up:\n\nCreates aÂ networkÂ (e.g.,Â foldername_default).\nStartsÂ dbÂ on that network with hostnameÂ db.\nStartsÂ apiÂ on the same network with hostnameÂ api.\nSets env vars inside each container as defined.\nPublishes host ports:\n\nHostÂ 5432Â â†’ containerÂ db:5432.\nHostÂ 8080Â â†’ containerÂ api:8080.\n\n\n\nServiceâ€‘toâ€‘service communication:\n\napiÂ reaches the database usingÂ DB_HOST=dbÂ (Dockerâ€‘provided DNS name).\nYou donâ€™t care about the actual container IPs.\n\nFrom your host:\n\npsql -h localhost -p 5432 -U orders_user orders\ncurl http://localhost:8080/actuator/health\n\nThis is exactly the networking and volumes mental model you already built, but declared in YAML instead of manualÂ docker runÂ flags.\n\n4. Developer workflows\nCompose gives you a few key commands that cover almost all daily needs.\nAssume you haveÂ docker-compose.ymlÂ in the current directory.\n4.1 Bring up the stack\ndocker compose up -d\n\n-dÂ runs in detached mode.\nCreates the network and volumes if they donâ€™t exist.\nStarts all services.\n\nTo see whatâ€™s running:\ndocker compose ps\n4.2 Logs\nTo see logs for all services:\ndocker compose logs\n# or follow:\ndocker compose logs -f\n\nFor just the API:\nbash\ndocker compose logs -f api\n4.3 Restarting services\nIf youâ€™ve rebuilt the image or changed configuration:\nbash\ndocker compose restart api\nThis restartsÂ onlyÂ theÂ apiÂ service, leavingÂ dbÂ and others alone.\nWhen code changes and you rebuild the image:\ndocker build -t myorg/orders-api:1.0.1 .\ndocker compose up -d api\n\nCompose compares the new image and recreates just that service.\n4.4 Stopping and cleaning up\nTo stop containers but keep volumes/networks:\nbash\ndocker compose down\nTo also remove volumes (careful: data loss for DB):\nbash\ndocker compose down -v\nIn dev:\n\nUseÂ docker compose downÂ when you just want to stop the stack.\nUseÂ -vÂ when you want a completely fresh environment (fresh DB, etc.).\n\n\n5. Patterns and best practices\n5.1 Separate override files for local dev vs CI\nCompose supports multiple files:\n\nBase file:Â docker-compose.ymlÂ (common definition).\nOverrides:Â docker-compose.override.yml,Â docker-compose.dev.yml, etc.\n\nExample:\ndocker-compose.ymlÂ (base):\nservices:\n  api:\n    image: myorg/orders-api:1.0.0\n    environment:\n      DB_HOST: db\n  db:\n    image: postgres:15-alpine\n    environment:\n      POSTGRES_PASSWORD: secret\n    volumes:\n      - pgdata:/var/lib/postgresql/data\n\nvolumes:\n  pgdata:\n\n\ndocker-compose.override.ymlÂ (local dev specifics):\nservices:\n  api:\n    build: .\n    image: myorg/orders-api:dev\n    ports:\n      - &quot;8080:8080&quot;\n  db:\n    ports:\n      - &quot;5432:5432&quot;\n\nThen:\ndocker compose up -d\nautomatically applies both base and override. In CI, you might use only the base file or a different override (e.g., no host port exposure).\nThis pattern:\n\nKeeps environmentâ€‘independent config in one place.\nKeeps environmentâ€‘specific tweaks (ports, build vs image, debug tools) in overrides.\n\n5.2 Healthchecks in Compose for better startup order\nComposeâ€™sÂ depends_onÂ only ensures startÂ order, not readiness. Your DB may start its process but not yet be ready to accept connections.\nYou can define healthchecks at the service level:\nservices:\n  db:\n    image: postgres:15-alpine\n    environment:\n      POSTGRES_PASSWORD: secret\n    healthcheck:\n      test: [&quot;CMD-SHELL&quot;,&quot;pg_isready -U postgres&quot;]\n      interval: 10s\n      timeout: 3s\n      retries: 5\n\n  api:\n    image: myorg/orders-api:1.0.0\n    depends_on:\n      db:\n        condition: service_healthy\n\nNow:\n\nCompose waits untilÂ dbÂ is markedÂ healthyÂ before startingÂ api.\nThis avoids common â€œAPI canâ€™t connect to DB on startupâ€ races in local dev.\n\n5.3 Avoid reâ€‘encoding docker run flags\nLet Compose own most of the configuration:\n\nPut ports, env, volumes, networks into YAML, not CLI flags.\nUseÂ docker composeÂ commands instead ofÂ docker runÂ for those services.\n\ndocker runÂ is still fine for oneâ€‘off debug containers (alpine shells, tools). For yourÂ stack, Compose should be the source of truth.\n\n6. Bridge to Kubernetes\nCompose is conceptually very close to how you define workloads in Kubernetes; the vocabulary changes, but the mental model stays.\nMapping:\n\nComposeÂ serviceÂ â†’ KubernetesÂ DeploymentÂ (or StatefulSet) +Â Service.\nComposeÂ volumesÂ â†’ KubernetesÂ PersistentVolumesÂ andÂ PersistentVolumeClaims.\nComposeÂ networksÂ â†’ Kubernetes cluster network and Service discovery (DNS names).\nComposeÂ environment variablesÂ â†’ Kubernetes Pod env vars.\n\nExample concept mapping forÂ apiÂ +Â db:\n\nservices.api:\n\nimage: myorg/orders-api:1.0.0Â â†’Â Deployment.spec.template.spec.containers[0].image.\nports: &quot;8080:8080&quot;Â â†’Â ServiceÂ exposing port 8080 externally.\nenvironmentÂ â†’ Pod env vars.\n\n\nservices.db:\n\nvolumes: pgdata:/var/lib/postgresql/dataÂ â†’ PVC + volume mount.\nhealthcheckÂ â†’ liveness/readiness probes.\n\n\n\nSo by getting comfortable with:\n\nDeclaring services, volumes, and networks in YAML.\nUsing service names instead of IPs.\nManaging multiâ€‘container lifecycles with a few commands.\n\nyouâ€™re building intuition that transfers almost 1:1 into Kubernetes manifests and Helm charts later."},"1-Devops/Docker-Mastery/Docker-Cheatsheet":{"slug":"1-Devops/Docker-Mastery/Docker-Cheatsheet","filePath":"1-Devops/Docker-Mastery/Docker Cheatsheet.md","title":"Docker Cheatsheet","links":[],"tags":[],"content":"Docker Cheatsheet â€“ Developer &amp; DevOps Reference\nThis page provides a concise, command-focused reference for common Docker tasks\nused by developers and DevOps engineers. It is intended for quick lookup during\ndevelopment, debugging, and operational troubleshooting.\nThe commands are grouped by use case and reflect practical, real-world workflows\nrather than exhaustive coverage.\nImages\n\nList images: docker images\nPull image: docker pull nginx:1.25-alpine\nBuild image: docker build -t myorg/app:1.0.0 .\nBuild no cache: docker build --no-cache -t myorg/app:1.0.0 .\nTag image: docker tag myorg/app:1.0.0 myorg/app:latest\nPush image: docker push myorg/app:1.0.0\nInspect image: docker inspect myorg/app:1.0.0\nImage history: docker history myorg/app:1.0.0\nPrune unused images: docker image prune -f\nPrune all unused: docker image prune -a -f\n\nContainers â€“ Run &amp; Lifecycle\n\nRun interactive: docker run --rm -it alpine:3.19 sh\nRun detached + port + name: docker run -d --name web -p 8080:80 nginx:1.25-alpine\nRun with limits: docker run -d --name web -p 8080:80 --cpus=&quot;1&quot; --memory=&quot;512m&quot; nginx:1.25-alpine\nRun with restart: docker run -d --restart=on-failure --name api -p 8080:8080 myorg/api:1.0.0\nList running: docker ps\nList all: docker ps -a\nStop container: docker stop web\nStart container: docker start web\nRemove container: docker rm web\nLogs follow: docker logs -f web\nExec shell: docker exec -it web sh\nInspect container: docker inspect web\nResource stats: docker stats\n\nNetworking\n\nList networks: docker network ls\nCreate bridge network: docker network create app-net\nRun on network (db):\ndocker run -d --name db --network app-net -e POSTGRES_PASSWORD=secret postgres:15-alpine\nRun on network (api):\ndocker run -d --name api --network app-net -e DB_HOST=db myorg/api:1.0.0\nConnect existing container: docker network connect app-net api\nInspect network: docker network inspect app-net\nHost network: docker run --net=host nginx:1.25-alpine\nNone network: docker run --network none alpine:3.19\n\nVolumes &amp; Persistence\n\nCreate volume: docker volume create pgdata\nList volumes: docker volume ls\nInspect volume: docker volume inspect pgdata\nPrune volumes: docker volume prune -f\nNamed volume (Postgres):\ndocker run -d --name db -e POSTGRES_PASSWORD=secret -v pgdata:/var/lib/postgresql/data postgres:15-alpine\nBind mount (dev):\ndocker run -d --name web-dev -p 3000:3000 -v &quot;$PWD/src:/usr/src/app&quot; node:22-alpine sh -c &quot;cd /usr/src/app &amp;&amp; npm install &amp;&amp; npm run dev&quot;\n\nDockerfile â€“ Core Snippets\n\nBase image: FROM eclipse-temurin:21-jre-alpine\nRun command: RUN apk add --no-cache curl\nCopy files: COPY app.jar /app/app.jar\nWorkdir: WORKDIR /app\nEnv var: ENV SPRING_PROFILES_ACTIVE=prod\nBuildâ€‘time arg:\nARG BUILD_VERSION=dev\nLABEL version=&quot;${BUILD_VERSION}&quot;\nExpose port: EXPOSE 8080\nNonâ€‘root user:\nRUN addgroup -S app &amp;&amp; adduser -S app -G app\nUSER app\nEntrypoint + cmd:\nENTRYPOINT [&quot;java&quot;,&quot;-jar&quot;,&quot;app.jar&quot;]\nCMD [&quot;--spring.profiles.active=prod&quot;]\nHealthcheck:\nHEALTHCHECK --interval=30s --timeout=3s CMD curl -f http://localhost:8080/actuator/health || exit 1\n\nMultiâ€‘Stage Builds\n\n\nNode example:\n   ```\n   FROM node:22-alpine AS build\n   WORKDIR /app\n   COPY package*.json ./\n   RUN npm ci\n   COPY . .\n   RUN npm run build\n\n   FROM nginx:1.25-alpine\n   COPY --from=build /app/dist /usr/share/nginx/html\n   EXPOSE 80\n   CMD [&quot;nginx&quot;,&quot;-g&quot;,&quot;daemon off;&quot;]\n\n   ```\n\n\n\nSecurity Essentials\n\nDrop root in Dockerfile: USER app (after creating user)\nDrop all caps, add minimal:\ndocker run -d --name web --cap-drop=ALL --cap-add=NET_BIND_SERVICE -p 80:80 nginx:1.25-alpine\nReadâ€‘only FS:\ndocker run -d --name api --read-only -v app-tmp:/tmp myorg/api:1.0.0\n\nRegistries &amp; Tagging\n\nLogin: docker login my-registry.example.com\nTag for registry:\ndocker tag myorg/api:1.0.0 my-registry.example.com/myorg/api:1.0.0\nPush: docker push my-registry.example.com/myorg/api:1.0.0\n\nDocker Compose â€“ Essentials\n\n\ndocker-compose.yml minimal:\n   ```\n   services:\n   db:\n      image: postgres:15-alpine\n      environment:\n         POSTGRES_PASSWORD: secret\n      volumes:\n         - pgdata:/var/lib/postgresql/data\n   api:\n      image: myorg/orders-api:1.0.0\n      ports:\n         - &quot;8080:8080&quot;\n      environment:\n         DB_HOST: db\n   volumes:\n   pgdata:\n\n   ```\n\n\n\nBring up: docker compose up -d\n\n\nStatus: docker compose ps\n\n\nLogs: docker compose logs -f api\n\n\nTear down: docker compose down\n\n\nTroubleshooting Quickies\n\nShell into container: docker exec -it api sh\nCheck env: docker inspect api\nCheck mounts: docker inspect api\nCheck ports: docker ps\nTest from host: curl http://localhost:8080/actuator/health\n"},"1-Devops/Docker-Mastery/index":{"slug":"1-Devops/Docker-Mastery/index","filePath":"1-Devops/Docker-Mastery/_index.md","title":"Docker Mastery","links":["1-Devops/Docker-Mastery/0-course-overview","1-Devops/Docker-Mastery/1-docker-images","1-Devops/Docker-Mastery/2-docker-containers","1-Devops/Docker-Mastery/3-docker-networking","1-Devops/Docker-Mastery/4-docker-volumes","1-Devops/Docker-Mastery/5-dockerfile-mastery","1-Devops/Docker-Mastery/6-multi-stage-docker-builds","1-Devops/Docker-Mastery/7-docker-security-essentials","1-Devops/Docker-Mastery/8-docker-registries-and-tagging","1-Devops/Docker-Mastery/9-docker-compose","1-Devops/Docker-Mastery/10-debugging-docker","1-Devops/Docker-Mastery/11-from-docker-to-orchestrators","1-Devops/","/"],"tags":[],"content":"Docker Mastery is a structured learning track and technical reference for\r\ndevelopers and DevOps engineers who want a deep, practical understanding of\r\nDocker.\n\nğŸ“š Course Modules\n\nCourse Overview &amp; Index\nDocker Images â€“ Layers &amp; Caching\nDocker Containers â€“ Linux Processes\nDocker Networking\nDocker Volumes\nDockerfile Mastery\nMulti-Stage Docker Builds\nDocker Security Essentials\nDocker Registries &amp; Tagging\nDocker Compose\nDebugging Docker\nFrom Docker to Orchestrators\n\n\nğŸ§­ How to Use This Course\n\n\nNew to Docker?\nFollow the modules in order for a complete learning path.\n\n\nUsing Docker at work?\nJump directly to the topic you needâ€”each module stands on its own.\n\n\nPreparing for Kubernetes?\nPay special attention to networking, volumes, image design, and Compose.\n\n\nThis course is designed to be read once, referenced often, and revisited as\r\nyour systems grow more complex.\n\nâ† Back to DevOps Track\nâ† Back to Courses Home"},"1-Devops/Kubernetes-Mastery/index":{"slug":"1-Devops/Kubernetes-Mastery/index","filePath":"1-Devops/Kubernetes-Mastery/_index.md","title":"Kubernetes Mastery","links":["1-Devops/Docker-Mastery/","1-Devops/","/"],"tags":[],"content":"â˜¸ï¸ Kubernetes Mastery\nKubernetes is not just YAML â€” it is a distributed systems platform for\r\nrunning containerized workloads reliably at scale.\nThis course focuses on how Kubernetes behaves, why it exists, and how to\r\nreason about it operationally â€” especially if you already understand Docker.\n\nğŸ¯ Course Goals\nBy the end of this course, you will be able to:\n\nUnderstand Kubernetes core primitives and their responsibilities\nReason about scheduling, scaling, and self-healing\nDebug real-world Kubernetes issues methodically\nConnect Docker mental models to Kubernetes abstractions\n\n\nğŸ“˜ Modules (Planned)\n1ï¸âƒ£ Kubernetes Architecture &amp; Mental Model\n\nWhy Kubernetes exists\nControl plane vs worker nodes\nAPI server, scheduler, controller manager\nDeclarative desired state\n\n\n2ï¸âƒ£ Pods &amp; Workloads\n\nPods and containers\nDeployments, ReplicaSets, Jobs, CronJobs\nRolling updates and restarts\nRestart and failure behavior\n\n\n3ï¸âƒ£ Networking &amp; Services\n\nCluster networking basics\nServices (ClusterIP, NodePort, LoadBalancer)\nDNS inside the cluster\nIngress and traffic flow (high level)\n\n\n4ï¸âƒ£ Configuration &amp; Secrets\n\nConfigMaps vs Secrets\nEnvironment variables vs mounted files\nConfiguration patterns that scale\n\n\n5ï¸âƒ£ Storage &amp; Persistence\n\nVolumes in Kubernetes\nPersistentVolumes and PVCs\nStateful workloads basics\n\n\n6ï¸âƒ£ Scaling &amp; Reliability\n\nHorizontal Pod Autoscaling\nResource requests and limits\nReadiness vs liveness probes\nSelf-healing behavior\n\n\n7ï¸âƒ£ Debugging &amp; Operations\n\nInspecting Pods and workloads\nLogs and exec\nCommon failure modes\nMapping Docker debugging skills to Kubernetes\n\n\nğŸ§­ Navigation\n\nğŸš§ This course is coming soon\nğŸ‘‰ Recommended prerequisite: Docker Mastery\n\nâ†’ Go to Docker Mastery\nâ† Back to DevOps &amp; Cloud\nâ† Back to Courses Home"},"1-Devops/Linux-Mastery/index":{"slug":"1-Devops/Linux-Mastery/index","filePath":"1-Devops/Linux-Mastery/_index.md","title":"Linux Mastery","links":["1-Devops/Docker-Mastery/","1-Devops/","/"],"tags":[],"content":"ğŸ§ Linux Mastery\nLinux is the foundation of modern DevOps, cloud platforms, and containers.\r\nIf you run software in production, Linux is always part of the picture â€” whether\r\nyou see it directly or not.\nThis course is designed to build practical Linux fluency for developers and\r\nDevOps engineers, focusing on how Linux behaves in real systems rather than\r\nmemorizing commands in isolation.\n\nğŸ¯ Course Goals\nBy the end of this course, you will be able to:\n\nNavigate Linux systems confidently\nUnderstand how processes, files, and permissions really work\nDebug common system-level issues\nReason about Linux behavior inside containers and cloud VMs\n\n\nğŸ“˜ Modules (Planned)\n1ï¸âƒ£ Linux Fundamentals\n\nFilesystem layout (/, /etc, /var, /home, /proc)\nEssential commands (cd, ls, cp, mv, rm, cat, less)\nWorking with files and directories safely\n\n\n2ï¸âƒ£ Text Processing &amp; CLI Power Tools\n\ngrep, awk, sed\nPipes and redirection\ntee, xargs, command substitution\nReading logs efficiently\n\n\n3ï¸âƒ£ Process &amp; Resource Management\n\nProcesses vs threads\nps, top, htop\nSignals and kill\nCPU and memory usage basics\n\n\n4ï¸âƒ£ Users, Groups &amp; Permissions\n\nUsers and groups\nFile permissions and ownership\nchmod, chown, umask\nRunning services securely\n\n\n5ï¸âƒ£ Networking Basics\n\nIP addresses, ports, and interfaces\nss, netstat, ping, curl\nDNS resolution basics\nDebugging connectivity issues\n\n\n6ï¸âƒ£ Linux for Containers &amp; Cloud\n\nLinux namespaces and cgroups (conceptual)\nFilesystems and mounts\nWhat Docker and Kubernetes rely on from Linux\n\n\nğŸ§­ Navigation\n\nğŸš§ This course is coming soon\nğŸ‘‰ Start with Docker Mastery to see Linux concepts in action\n\nâ†’ Go to Docker Mastery\nâ† Back to DevOps &amp; Cloud\nâ† Back to Courses Home"},"1-Devops/index":{"slug":"1-Devops/index","filePath":"1-Devops/_index.md","title":"DevOps & Cloud","links":["1-Devops/Linux-Mastery/","1-Devops/Docker-Mastery/","1-Devops/Kubernetes-Mastery/","/"],"tags":[],"content":"âš™ï¸ DevOps &amp; Cloud\nDevOps is about owning software beyond writing code.\nThis track focuses on how applications behave after they leave your laptop:\r\nhow they run, how they fail, how they scale, and how they are operated in real\r\nproduction environments.\nYouâ€™ll build strong mental models around:\n\nHow applications are packaged and deployed\nHow infrastructure and runtime concerns affect reliability\nHow modern DevOps practices reduce risk and increase velocity\n\n\nğŸ“˜ Courses\nğŸ”¹ Linux Mastery (planned)\nA practical foundation for anyone working with servers, containers, or cloud\r\ninfrastructure.\nWhat youâ€™ll learn:\n\nLinux basic commands (cd, mkdir, mv, cat, vim, and more)\nAdvanced command-line tools (top, df, du, grep, tee, etc.)\nLinux administration basics:\n\nUsers and groups\nPermissions and ownership\nProcesses and services\nNetworking fundamentals\n\n\n\nğŸ‘‰ Linux Mastery (Coming Soon) â†’\n\nğŸ”¹ Docker Mastery\nA deep, engineer-level understanding of Docker â€” from internals to\r\nproduction-ready usage.\nThis course emphasizes how Docker really works, not just how to run commands.\nWhat youâ€™ll learn:\n\nImages, containers, networking, and volumes\nWriting clean and efficient Dockerfiles\nMulti-stage builds and image optimization\nSecurity best practices\nHow Docker concepts map directly to Kubernetes\n\nğŸ‘‰ Start Docker Mastery â†’\n\nğŸ”¹ Kubernetes Mastery (planned)\nAn operations-focused path to understanding Kubernetes beyond YAML.\nWhat youâ€™ll learn:\n\nCore primitives: Pods, Deployments, Services\nScheduling, scaling, and self-healing\nConfiguration, secrets, and networking\nObservability and systematic troubleshooting\n\nğŸ‘‰ Kubernetes Mastery (Coming Soon) â†’\n\nâ† Back to Courses Home"},"2-Backend/index":{"slug":"2-Backend/index","filePath":"2-Backend/_index.md","title":"Backend Engineering","links":["/"],"tags":[],"content":"ğŸ§  Backend Engineering\nThis track is about thinking like a backend engineer, not just writing APIs.\nFocus areas:\n\nSystem design thinking\nClean architecture\nPerformance &amp; reliability\n\n\nğŸ“˜ Courses\nğŸ”¹ Java &amp; Spring Boot Mastery (planned)\n\nCore Java internals\nSpring Boot deep dive\nREST &amp; API design\nObservability &amp; resilience\n\nğŸ”¹ Microservices Architecture (planned)\n\nService boundaries\nCommunication patterns\nData consistency\nFailure handling\n\n\nâ† Back to Courses Home"},"3-Frontend/index":{"slug":"3-Frontend/index","filePath":"3-Frontend/_index.md","title":"Frontend Engineering","links":["/"],"tags":[],"content":"ğŸ¨ Frontend Engineering\nThis track focuses on building maintainable, high-performance user interfaces.\nIt emphasizes:\n\nFundamentals before frameworks\nEngineering trade-offs\nReal-world UI architecture\n\n\nğŸ“˜ Courses\nğŸ”¹ Frontend Foundations (planned)\n\nHTML semantics &amp; accessibility\nCSS layout systems (Flexbox, Grid)\nModern JavaScript fundamentals\n\nğŸ”¹ Angular in Depth (planned)\n\nComponent architecture\nChange detection\nState management\nPerformance tuning\n\n\nâ† Back to Courses Home"},"4-AI-ML/index":{"slug":"4-AI-ML/index","filePath":"4-AI-ML/_index.md","title":"AI & Machine Learning","links":["/"],"tags":[],"content":"ğŸ¤– AI &amp; Machine Learning\nThis track focuses on understanding AI systems, not just using libraries.\nEmphasis:\n\nClear mental models\nPractical applications\nEngineering perspective\n\n\nğŸ“˜ Courses\nğŸ”¹ Pre-Requisites (planned)\n\nSQL In-Depth\nTableu\nPython In-Depth and many more..\n\nğŸ”¹ Machine Learning Foundations (planned)\n\nML concepts &amp; intuition\nData pipelines\nModel evaluation\n\nğŸ”¹ Generative AI &amp; LLMs (planned)\n\nHow LLMs work\nPrompting &amp; tooling\nReal-world integration patterns\n\n\n.. And more topics to be added soon !\n\nâ† Back to Courses Home"},"index":{"slug":"index","filePath":"index.md","title":"Courses","links":["3-Frontend/","2-Backend/","1-Devops/","4-AI-ML/"],"tags":[],"content":"\nğŸ”— Links\nğŸ‘‰ â† Back to Resume / Portfolio\n\nFull-Stack-Dev Courses\nWelcome to my curated learning space.\nThese courses are designed to:\n\nBuild strong mental models\nFocus on real-world engineering\nMove beyond tutorials into professional-grade understanding\n\n\nğŸš€ Course Tracks\n\n\nğŸ¨ Frontend Engineering\nBuild modern, scalable user interfaces with a strong foundation in fundamentals and frameworks.\nTopics include: HTML, CSS, JavaScript, Angular, performance, and architecture.\n\n\nğŸ‘‰ Explore Frontend â†’\n\n\nğŸ§  Backend Engineering\nDesign robust backends, APIs, and distributed systems with production-grade patterns.\nTopics include: Java, Spring Boot, Microservices, Databases, Messaging.\n\n\nğŸ‘‰ Explore Backend â†’\n\n\nâš™ï¸ DevOps &amp; Cloud\nUnderstand how software runs in the real world â€” containers, CI/CD, orchestration, and infrastructure.\nTopics include: Docker, Kubernetes, Linux, CI/CD, Cloud fundamentals.\n\n\nğŸ‘‰ Explore DevOps â†’\n\n\nğŸ¤– AI &amp; Machine Learning\nBuild intuition for AI systems, ML pipelines, and modern applied AI.\nTopics include: ML basics, GenAI, LLMs, tooling, and real-world use cases.\n\n\nğŸ‘‰ Explore AI - ML â†’\n\nğŸ“Œ How to use this site\n\nEach track has a course index\nEach course is broken into logical modules\nYou can always return here using Home links\n\nHappy learning ğŸš€"}}